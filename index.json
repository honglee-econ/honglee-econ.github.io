[{"authors":["admin"],"categories":null,"content":"I am an Assistant Professor at the Department of Economics, Walker College of Business, Appalachian State University. I am also a Research Fellow of the Knee Center for the Study of Occupational Regulation at West Virginia University.\nI am an applied economist specializing in healthcare provider labor, licensing, and regulation markets. I am passionate about finding solutions that improve healthcare access and quality at reduced costs.\nI use various applied econometric, spatial, and policy evaluation techniques and supplement my empirical knowledge with the practice of technology-driven methods such as data-scrapping, big data, predictive, and causal machine learning approaches.\nI have published in numerous peer-reviewed academic journals, including the World Development, Annals of Regional Science, Energy Economics, Contemporary Economic Policy, Drug and Alcohol Dependence, Spatial Economic Analysis, Applied Economics Letters, Journal of Labor Research, and others.\nI will teach Business and Economic Statistics II and Timeseries Forecasting in Fall 2023. Previously, I have taught Regional Economics, Economic Analysis of Big Data, Principles of Economics, Macroeconomics, Managerial Economics, Business Data Visualization, and Elementary Business \u0026amp; Economic Statistics.\nFor any questions about my research or other scholarly activities, please get in touch with me by email shakyas@appstate.edu. You can also reach me on Twitter @econshishir.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ShishirShakya.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am an Assistant Professor at the Department of Economics, Walker College of Business, Appalachian State University. I am also a Research Fellow of the Knee Center for the Study of Occupational Regulation at West Virginia University.\nI am an applied economist specializing in healthcare provider labor, licensing, and regulation markets. I am passionate about finding solutions that improve healthcare access and quality at reduced costs.\nI use various applied econometric, spatial, and policy evaluation techniques and supplement my empirical knowledge with the practice of technology-driven methods such as data-scrapping, big data, predictive, and causal machine learning approaches.","tags":null,"title":"Shishir Shakya","type":"authors"},{"authors":null,"categories":["Product Life Cycle"],"content":"\rThe Bass Diffusion model\rThe Bass Diffusion model elaborates on how a new product diffusion occurs in society and, therefore, can help to analyze the innovation diffusion.\nThis model is very interesting. It uses some assumption that relates to consumer’s behavior, then, based on those assumptions, develops a model in functional form. Then, one can fit the real data on that functional form to solve the parameters.\nThis model has three assumptions:\nThere is no repetitive purchase or consumer purchase for one time i.e., consumer durable good;\nThere is no supply shock and;\nThe probability that an initial purchase at time \\(T\\) i.e., given that no purchase has yet made \\(P\\left( T \\right)~\\), is a linear function of the number of previous buyers.\n\\[P\\left( T \\right)~~~=\\text{ }p\\text{ }+\\text{ }\\left( q/m \\right)Y\\left( T \\right)\\to (1)\\]\nWhere \\(p\\) and \\(q/m\\) are constants. \\(Y\\left( T \\right)\\) is the number of previous buyers.\nFor \\(T=0\\), \\(Y\\left( 0 \\right)=0\\), then \\(p\\) represents the probability of an initial purchase at \\(T=0\\), and its magnitude reflects the importance of innovators in the social system. The \\(p\\) is the fraction of all adopter who adopts products as it launches. These are the people who buy the product as the product appear in the market.\nLet’s define \\(m\\) as the size of potential market size, then \\((q/m)\\) is the portion of the market that has not yet purchased the product.\n\\((q/m)\\) times \\(Y(T )\\) reflects the pressures operating on imitators as the number of previous buyers increases.\nInterestingly, \\(Y(T)/m\\) is portion of total purchase at time \\(T\\). Let’s denote this by a function as \\(F(t)\\). i.e\n\\[\\frac{Y(T)}{m}=F(T)\\]\nLet’s denote the likelihood of purchase at \\(T\\)as\\(f(t)\\), then we can write \\(F(t)=\\int\\limits_{0}^{T}{f(t)dt}\\).\nNow, by definition for \\(p(T)\\) (The probability that an initial purchase will be made at \\(T\\) i.e., given that no purchase has yet been made) can be written as:\n\\[\\frac{f(T)}{1-F(T)}=P(T)=p+(q/m)Y(T)=p+qF(T)\\]\nNow, let’s solve for \\(f(t)\\) only,\n\\[\\begin{align}\r\u0026amp; f(T)=[p+qF(T)][1-F(T)] \\\\\r\u0026amp; or, f(T)=p-pF(T)+qF(T)-q{{[F(T)]}^{2}} \\\\\r\u0026amp; or, f(T)=p+(q-p)F(T)-q{{[F(T)]}^{2}} \\\\\r\u0026amp; or, f(T)=\\frac{dF}{dt}=p+(q-p)F(T)-q{{[F(T)]}^{2}} \\\\\r\u0026amp; \\therefore {F}\u0026#39;(T)=p+(q-p)F(T)-q{{[F(T)]}^{2}} \\\\\r\\end{align}\\]\nThis above equation is a nonlinear differential equation. Before I begin to solve this, please kindly open the visualization externally in Wolfram Mathematica Cloud.\nPlease change the sliders \\(p\\), \\(q\\), and \\(m\\) and see what happens to the curve. The \\(p\\), \\(q\\), and \\(m\\) are coefficient of innovation, coefficient of imitation, and potential size of the market.\nLet’s do some nonlinear econometrics and estimate the values of \\(m\\), \\(p\\) and \\(q\\) based upon the real dataset using R.\nClean the environment.\rrm(list = ls())\rdev.off(dev.list()[\u0026quot;RStudioGD\u0026quot;])\rInstall required packages.\r# Load all the required packages\ripak \u0026lt;- function(pkg){\rnew.pkg \u0026lt;- pkg[!(pkg %in% installed.packages()[, \u0026quot;Package\u0026quot;])]\rif (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE, repos = \u0026quot;http://cran.us.r-project.org\u0026quot;)\rsapply(pkg, require, character.only = TRUE)\r}\ripak(c(\u0026quot;nlstools\u0026quot;, \u0026quot;fBasics\u0026quot;, \u0026quot;tseries\u0026quot;, \u0026quot;rstudioapi\u0026quot;, \u0026quot;forecast\u0026quot;))\r## nlstools fBasics tseries rstudioapi forecast ## TRUE TRUE TRUE TRUE TRUE\rSetup Working Directory.\rpath \u0026lt;- dirname(rstudioapi::getActiveDocumentContext()$path)\rsetwd(path)\r","date":1582243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582243200,"objectID":"96637244891f30fa56e25ec90b75ce35","permalink":"https://ShishirShakya.github.io/post/2020-02-21-product-life-cycle/","publishdate":"2020-02-21T00:00:00Z","relpermalink":"/post/2020-02-21-product-life-cycle/","section":"post","summary":"The Bass Diffusion model\rThe Bass Diffusion model elaborates on how a new product diffusion occurs in society and, therefore, can help to analyze the innovation diffusion.\nThis model is very interesting. It uses some assumption that relates to consumer’s behavior, then, based on those assumptions, develops a model in functional form. Then, one can fit the real data on that functional form to solve the parameters.\nThis model has three assumptions:","tags":["Time series,","Theory,","Prediction,"],"title":"Product Diffusion Model","type":"post"},{"authors":null,"categories":["Non Parametric Econometrics"],"content":"\rUnivariate Density Estimation\rParametric Desity Estimation\rDraw from a normal distribution\rGiven \\({{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}}\\) \\(i.i.d\\) draw from a normal distribution with mean of \\(\\mu\\) and variance of \\({{\\sigma }^{2}}\\) the joint \\(PDF\\) can be expressed as:\r\\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\prod\\limits_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}}\\]\r\\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\cdots \\times \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{n}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\]\rThe term \\(\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}\\) is a constant multiplying this term for \\(n\\) times gives \\({{\\left( \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}} \\right)}^{n}}=\\frac{1}{{{\\left( 2\\pi \\sigma \\right)}^{\\frac{n}{2}}}}\\).\r\\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\frac{1}{{{\\left( 2\\pi \\sigma \\right)}^{\\frac{n}{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times {{e}^{-\\frac{{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\cdots \\times {{e}^{-\\frac{{{\\left( {{X}_{n}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\]\rWith the index law of addition i.e. we can add the indices for the same base\r\\[{{e}^{-\\frac{{{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times {{e}^{-\\frac{{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\cdots \\times {{e}^{-\\frac{{{\\left( {{X}_{n}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}={{e}^{-\\frac{1}{2{{\\sigma }^{2}}}\\left\\{ {{\\left( {{X}_{1}}-\\mu \\right)}^{2}}+{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}+\\cdots +{{\\left( {{X}_{n}}-\\mu \\right)}^{2}} \\right\\}}}={{e}^{-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\limits_{i=1}^{n}{{{\\left( Xi-\\mu \\right)}^{2}}}}}\\]\rTherefore,\r\\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\frac{1}{{{\\left( 2\\pi {{\\sigma }^{2}} \\right)}^{\\frac{n}{2}}}}{{e}^{-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}}}\\]\nThe log-likelihood function\rTaking the logarithm, we get the log-likelihood function as:\r\\[\\ln f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\ln \\left[ \\frac{1}{{{\\left( 2\\pi {{\\sigma }^{2}} \\right)}^{\\frac{n}{2}}}}{{e}^{-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}}} \\right]\\]\nWith the property of log i.e. multiplication inside the log can be turned into addition outside the log, and vice versa or \\(\\ln (ab)=\\ln (a)+\\ln (b)\\)\r\\[L\\left( \\mu ,{{\\sigma }^{2}} \\right)\\equiv \\ln \\left( \\frac{1}{{{\\left( 2\\pi {{\\sigma }^{2}} \\right)}^{\\frac{n}{2}}}} \\right)+\\ln \\left( {{e}^{-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}}} \\right)\\]\rWith natural log property i.e. when \\({{e}^{y}}=x\\) then \\(\\ln (x)=\\ln ({{e}^{y}})=y\\)\r\\[L\\left( \\mu ,{{\\sigma }^{2}} \\right)\\equiv \\ln \\left( {{\\left( 2\\pi {{\\sigma }^{2}} \\right)}^{-\\frac{n}{2}}} \\right)-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\]\r\\[L\\left( \\mu ,{{\\sigma }^{2}} \\right)\\equiv -\\frac{n}{2}\\ln \\left( 2\\pi \\right)-\\frac{n}{2}\\ln {{\\sigma }^{2}}-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\]\r\\[L\\left( \\mu ,{{\\sigma }^{2}} \\right)\\equiv \\ln f\\left( {{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}};\\mu ,{{\\sigma }^{2}} \\right)=-\\frac{n}{2}\\ln \\left( 2\\pi \\right)-\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right)-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\]\nLogliklihood function optimization\rTo find the optimum value of\r\\(L\\left( \\mu ,{{\\sigma }^{2}} \\right)\\equiv \\ln f\\left( {{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}};\\mu ,{{\\sigma }^{2}} \\right)=-\\frac{n}{2}\\ln \\left( 2\\pi \\right)-\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right)-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\), we take the first order condition w.r.t. \\(\\mu\\) and \\({{\\sigma }^{2}}\\). The necessary first order condition w.r.t \\(\\mu\\) is given as:\r\\[\\frac{\\partial L\\left( \\mu ,{{\\sigma }^{2}} \\right)}{\\partial \\mu }=\\frac{\\partial }{\\partial \\mu }\\left\\{ -\\frac{n}{2}\\ln \\left( 2\\pi \\right)-\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right)-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}} \\right\\}=0\\]\rHere, \\(\\frac{\\partial }{\\partial \\mu }\\left\\{ -\\frac{n}{2}\\ln \\left( 2\\pi \\right) \\right\\}=0\\) and \\(\\frac{\\partial }{\\partial \\mu }\\left\\{ -\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right) \\right\\}=0\\), so we only need to solve for\r\\[\\frac{\\partial }{\\partial \\mu }\\left\\{ -\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}} \\right\\}=0\\]\r\\[-\\frac{1}{2{{\\sigma }^{2}}}\\frac{\\partial }{\\partial \\mu }\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}=0\\]\r\\[-\\frac{1}{2{{\\sigma }^{2}}}\\left[ \\frac{\\partial }{\\partial \\mu }\\left\\{ {{\\left( {{X}_{1}}-\\mu \\right)}^{2}}+{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}+\\cdots +{{\\left( {{X}_{n}}-\\mu \\right)}^{2}} \\right\\} \\right]=0\\]\r\\[-\\frac{1}{2{{\\sigma }^{2}}}\\left[ \\frac{\\partial {{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{\\partial \\mu }+\\frac{\\partial {{\\left( {{X}_{2}}-\\mu \\right)}^{2}}}{\\partial \\mu }+\\cdots +\\frac{\\partial {{\\left( {{X}_{n}}-\\mu \\right)}^{2}}}{\\partial \\mu } \\right]=0\\]\rWith the chain rule i.e \\(\\frac{\\partial {{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{\\partial \\mu }=\\frac{\\partial {{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{\\partial \\left( {{X}_{1}}-\\mu \\right)}\\frac{\\partial \\left( {{X}_{1}}-\\mu \\right)}{\\partial \\mu }=2\\left( {{X}_{1}}-\\mu \\right)\\left( 1 \\right)=2\\left( {{X}_{1}}-\\mu \\right)\\). Hence,\r\\[-\\frac{1}{2{{\\sigma }^{2}}}\\left[ 2\\left( {{X}_{1}}-\\mu \\right)+2\\left( {{X}_{2}}-\\mu \\right)+\\cdots +2\\left( {{X}_{n}}-\\mu \\right) \\right]=0\\]\r\\[-\\frac{1}{{{\\sigma }^{2}}}\\sum\\limits_{i=1}^{n}{\\left( {{X}_{i}}-\\mu \\right)}=0\\]\rSince \\({{\\sigma }^{2}}\\ne 0\\), So,\r\\[\\sum\\limits_{i=1}^{n}{\\left( {{X}_{i}}-\\mu \\right)}=0\\]\r\\[\\sum\\limits_{i=1}^{n}{{{X}_{i}}}-\\sum\\limits_{i=1}^{n}{\\mu }=0\\]\r\\[\\sum\\limits_{i=1}^{n}{{{X}_{i}}}-n\\mu =0\\]\r\\[\\hat{\\mu }={{n}^{-1}}\\sum\\limits_{i=1}^{n}{{{X}_{i}}}\\]\nThe necessary first order condition w.r.t \\({{\\sigma }^{2}}\\) is given as:\n\\[\\frac{\\partial L\\left( \\mu ,{{\\sigma }^{2}} \\right)}{\\partial {{\\sigma }^{2}}}=\\frac{\\partial }{\\partial {{\\sigma }^{2}}}\\left\\{ -\\frac{n}{2}\\ln \\left( 2\\pi \\right)-\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right)-\\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}} \\right\\}=0\\]\n\\[-\\frac{\\partial }{\\partial {{\\sigma }^{2}}}\\left\\{ \\frac{n}{2}\\ln \\left( 2\\pi \\right) \\right\\}-\\frac{\\partial }{\\partial {{\\sigma }^{2}}}\\left\\{ -\\frac{n}{2}\\ln \\left( {{\\sigma }^{2}} \\right) \\right\\}-\\frac{\\partial }{\\partial {{\\sigma }^{2}}}\\left\\{ \\frac{1}{2{{\\sigma }^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}} \\right\\}=0\\]\n\\[0-\\frac{n}{2}\\frac{1}{{{\\sigma }^{2}}}-\\frac{1}{2}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\frac{\\partial {{\\left( {{\\sigma }^{2}} \\right)}^{-1}}}{\\partial {{\\sigma }^{2}}}=0\\]\n\\[-\\frac{1}{2}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\left( -1 \\right){{\\left( {{\\sigma }^{2}} \\right)}^{-2}}=\\frac{n}{2{{\\sigma }^{2}}}\\]\n\\[\\frac{1}{2{{\\left( {{\\sigma }^{2}} \\right)}^{2}}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}=\\frac{n}{2{{\\sigma }^{2}}}\\]\n\\[{{\\hat{\\sigma }}^{2}}={{n}^{-1}}\\sum\\nolimits_{i=1}^{n}{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}\\]\n\\(\\hat{\\mu }\\) and \\({{\\hat{\\sigma }}^{2}}\\) above are the maximum likelihood estimator of \\(\\mu\\) and \\({{\\sigma }^{2}}\\), respectively, the resulting estimator of \\(f\\left( x \\right)\\) is:\r\\[\\hat{f}\\left( x \\right)=\\frac{1}{\\sqrt{2\\pi {{{\\hat{\\sigma }}}^{2}}}}{{e}^{\\left[ -\\frac{1}{2}\\left( \\frac{x-\\hat{\\mu }}{{{{\\hat{\\sigma }}}^{2}}} \\right) \\right]}}\\]\nSimulation example\rLet’s simulate 10000 random observation from a normal distribution with mean of 2 and standard deviation of 1.5. To reproduce the results we will use set.seed() function.\nset.seed(1234)\rN \u0026lt;- 10000\rmu \u0026lt;- 2\rsigma \u0026lt;- 1.5\rx \u0026lt;- rnorm(n = N, mean = mu, sd = sigma)\rWe can use mean() and sd() function to find the mean and sigma\n# mean\rsum(x)/length(x)\r## [1] 2.009174\rmean(x)\r## [1] 2.009174\r# standard deviation\rsqrt(sum((x-mean(x))^2)/(length(x)-1))\r## [1] 1.481294\rsd(x)\r## [1] 1.481294\rHowever, if can also simulate and try the optimization using the mle function from the stat 4 package in R.\nLL \u0026lt;- function(mu, sigma){\rR \u0026lt;- dnorm(x, mu, sigma)\r-sum(log(R))\r}\rstats4::mle(LL, start = list(mu = 1, sigma = 1))\r## ## Call:\r## stats4::mle(minuslogl = LL, start = list(mu = 1, sigma = 1))\r## ## Coefficients:\r## mu sigma ## 2.009338 1.481157\rTo supress the warnings in R and garanatee the solution we can use following codes.\nstats4::mle(LL, start = list(mu = 1, sigma = 1), method = \u0026quot;L-BFGS-B\u0026quot;,\rlower = c(-Inf, 0), upper = c(Inf, Inf))\r## ## Call:\r## stats4::mle(minuslogl = LL, start = list(mu = 1, sigma = 1), ## method = \u0026quot;L-BFGS-B\u0026quot;, lower = c(-Inf, 0), upper = c(Inf, Inf))\r## ## Coefficients:\r## mu sigma ## 2.009174 1.481221\rDensity plot example\rGiven the data of x = (-0.57, 0.25, -0.08, 1.40, -1.05, 1.00, 0.37, -1.15, 0.73, 1.59), we can estimate \\(\\hat{\\mu }\\) as sum(x)/length(x) or mean(x) and \\({{\\hat{\\sigma }}^{2}}\\) as sum((x-mean(x))^2)/(length(x)-1) or var(x). Note I use the sample variance formula.\nx \u0026lt;- c(-0.57, 0.25, -0.08, 1.40, -1.05, 1.00, 0.37, -1.15, 0.73, 1.59)\r# mean\rsum(x)/length(x)\r## [1] 0.249\rmean(x)\r## [1] 0.249\r# variance\rsum((x-mean(x))^2)/(length(x)-1)\r## [1] 0.9285211\rvar(x)\r## [1] 0.9285211\rWe can also plot a parametric density function. But prior we plot, we have to sort the data.\nx \u0026lt;- sort(x)\rplot(x ,dnorm(x,mean=mean(x),sd=sd(x)),ylab=\u0026quot;Density\u0026quot;,type=\u0026quot;l\u0026quot;, col = \u0026quot;blue\u0026quot;, lwd = 3)\rLet’s also plot a graph of histogram using bin width ranging from -1.5 through 2.0.\nhist(x, breaks=seq(-1.5,2,by=0.5), prob = TRUE)\rUnivariate NonParametric Density Estimation\rSet-up\rConsider \\(i.i.d\\) data \\({{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}}\\) with \\(F\\left( \\centerdot \\right)\\) an unknown \\(CDF\\) where \\(F\\left( x \\right)=P\\left[ X\\le x \\right]\\) or \\(CDF\\) of \\(X\\) evaluated at \\(x\\). We can do a na?ve estimation as \\(F\\left( x \\right)=P\\left[ X\\le x \\right]\\) as cumulative sums of relative frequency as:\n\\[{{F}_{n}}\\left( x \\right)={{n}^{-1}}\\left\\{ \\#\\ of\\ {{X}_{i}}\\le x \\right\\}\\] and \\(n\\to\\infty\\) yields \\({{F}_{n}}\\left( x \\right)\\to F\\left( x \\right)\\).\nThe \\(PDF\\) of \\(F\\left( x \\right)=P\\left[ X\\le x \\right]\\) is given as \\(f\\left( x \\right)=\\frac{d}{dx}F\\left( x \\right)\\) and an obvious estimator is:\r\\[\\hat{f}\\left( x \\right)=\\frac{rise}{run}=\\frac{{{F}_{n}}\\left( x+h \\right)-{{F}_{n}}\\left( x-h \\right)}{x+h-\\left( x-h \\right)}=\\frac{{{F}_{n}}\\left( x+h \\right)-{{F}_{n}}\\left( x-h \\right)}{2h}={{n}^{-1}}\\frac{1}{2h}\\left\\{ \\#\\ of\\ {{X}_{i}}\\ in\\ between\\ \\left[ x-h,x+h \\right] \\right\\}\\]\nNaive Kernel\rThe \\(k\\left( \\centerdot \\right)\\) can be any kernel function, If we define a uniform kernel function or also known as na?ve kernel function then\r\\[k\\left( z \\right)=\\left\\{ \\begin{matrix}\r{1}/{2}\\; \u0026amp; if\\ \\left| z \\right|\\le 1 \\\\\r0 \u0026amp; o.w \\\\\r\\end{matrix} \\right.\\]\rWhere \\(\\left| {{z}_{i}} \\right|=\\left| \\frac{{{X}_{i}}-x}{h} \\right|\\) and therefore is symmetric and hence \\(\\#\\ of\\ {{X}_{i}}\\ in\\ between\\ \\left[ x-h,x+h \\right]\\) means \\(2\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\). Then it is easy to see \\(\\hat{f}\\left( x \\right)\\) to be expressed as:\r\\[\\hat{f}\\left( x \\right)=\\frac{1}{2h}{{n}^{-1}}\\left\\{ \\#\\ of\\ {{X}_{i}}\\ in\\ between\\ \\left[ x-h,x+h \\right] \\right\\}=\\frac{1}{2h}{{n}^{-1}}\\sum\\limits_{i=1}^{n}{k\\left( 2\\frac{{{X}_{i}}-x}{h} \\right)}=\\frac{1}{nh}\\sum\\limits_{i=1}^{n}{k\\left( \\frac{{{X}_{i}}-x}{h} \\right)}\\]\nWe can use follwoing code for naive kernel.\nx \u0026lt;- c(-0.57, 0.25, -0.08, 1.40, -1.05, 1.00, 0.37, -1.15, 0.73, 1.59)\rx \u0026lt;- sort(x)\rnaive_kernel \u0026lt;- function(x,y,h){\rz \u0026lt;- (x-y)/h\rifelse(abs(z) \u0026lt;= 1, 1/2, 0)\r}\rnaive_density \u0026lt;- function(x, h){\rval \u0026lt;- c()\rfor(i in 1:length(x)) {\rval[i] \u0026lt;- sum(naive_kernel(x,x[i],h)/(length(x)*h))\r}\rval\r}\rH \u0026lt;- c(0.5, 1.0, 1.5)\rnames \u0026lt;- as.vector(paste0(\u0026quot;H = \u0026quot;, H))\rdensity_data \u0026lt;- list()\rfor(i in 1:length(H)){\rdensity_data[[i]] \u0026lt;- naive_density(x, H[i])\r}\rdensity_data \u0026lt;- do.call(cbind.data.frame, density_data)\rcolnames(density_data) \u0026lt;- names\rmatplot(density_data, type = \u0026quot;b\u0026quot;, xlab = \u0026quot;x\u0026quot;,\rylab = \u0026quot;Density\u0026quot;, pch=1:length(H), col = 1:length(H),\rmain = \u0026quot;Naive Density for various Smoothing Parameter H\u0026quot;)\rlegend(\u0026quot;topleft\u0026quot;, legend=names, bty = \u0026quot;n\u0026quot;, pch=1:length(H), col = 1:length(H))\rEpanechnikov kernel\rConsider another optimal kernel known as Epanechnikov kernel given by:\r\\[k\\left( \\frac{{{X}_{i}}-x}{h} \\right)=\\left\\{ \\begin{matrix}\r\\frac{3}{4\\sqrt{5}}\\left( 1-\\frac{1}{5}{{\\left( \\frac{{{X}_{i}}-x}{h} \\right)}^{2}} \\right) \u0026amp; if\\ \\left| \\frac{{{X}_{i}}-x}{h} \\right|\u0026lt;5 \\\\\r0 \u0026amp; o.w \\\\\r\\end{matrix} \\right.\\]\rLet’s use x = (-0.57, 0.25, -0.08, 1.40, -1.05, 1.00, 0.37, -1.15, 0.73, 1.59) and compute the kernel estimator of the density function of every sample realization using bandwidth of \\(h=0.5\\), \\(h=1\\), \\(h=1.5\\), where, \\(h\\) is smoothing parameter restricted to lie in the range of \\((0,\\infty ]\\). We can use follwoing codes to estimate the density using Epanechnikov kernel.\nx \u0026lt;- c(-0.57, 0.25, -0.08, 1.40, -1.05, 1.00, 0.37, -1.15, 0.73, 1.59)\rx \u0026lt;- sort(x)\repanichnikov_kernel \u0026lt;- function(x,y,h){\rz \u0026lt;- (x-y)/h\rifelse(abs(z) \u0026lt; sqrt(5), (1-z^2/5)*(3/(4*sqrt(5))), 0)\r}\repanichnikov_density \u0026lt;- function(x, h){\rval \u0026lt;- c()\rfor(i in 1:length(x)) {\rval[i] \u0026lt;- sum(epanichnikov_kernel(x,x[i],h)/(length(x)*h))\r}\rval\r}\rH \u0026lt;- c(0.5, 1.0, 1.5)\rnames \u0026lt;- as.vector(paste0(\u0026quot;H = \u0026quot;, H))\rdensity_data \u0026lt;- list()\rfor(i in 1:length(H)){\rdensity_data[[i]] \u0026lt;- epanichnikov_density(x, H[i])\r}\rdensity_data \u0026lt;- do.call(cbind.data.frame, density_data)\rcolnames(density_data) \u0026lt;- names\rmatplot(density_data, type = \u0026quot;b\u0026quot;, xlab = \u0026quot;x\u0026quot;,\rylab = \u0026quot;Density\u0026quot;, pch=1:length(H), col = 1:length(H),\rmain = \u0026quot;Epanichnikov Density for various Smoothing Parameter H\u0026quot;)\rlegend(\u0026quot;topleft\u0026quot;, legend=names, bty = \u0026quot;n\u0026quot;, pch=1:length(H), col = 1:length(H))\rThree properties of kernel estimator\rFor any general nonnegative bounded kernel function \\(k\\left( v \\right)\\) where \\(v=\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\), the kernel estimator \\(\\hat{f}\\left( x \\right)\\) is a consistent estimator of \\(f\\left( x \\right)\\) that satisfies three conditions:\rFirst is area under a kernel to be unity.\r\\[\\int{k(v)dv=1}\\]\nSecond is the symmetry kernel\r\\[\\int{vk(v)dv=0}\\] which implies symmetry condition i.e. \\(k(v)=k(-v)\\). For asymmetric kernels see Abadir and Lawford (2004).\nThird is a positive constant.\r\\[\\int{{{v}^{2}}k(v)dv={{\\kappa }_{2}}\u0026gt;0}\\]\nThe big O and small o.\rTaylor series expansion\rFor a univariate function \\(g(x)\\)evaluated at \\({{x}_{0}}\\) , we can express with Taylor series expansion as:\r\\[g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\\cdots +\\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\\frac{1}{(m)!}{{g}^{(m)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m}}+\\cdots \\]\rFor a univariate function \\(g(x)\\)evaluated at \\({{x}_{0}}\\) that is \\(m\\) times differentiable, we can express with Taylor series expansion as:\r\\[g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\\cdots +\\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\\frac{1}{(m)!}{{g}^{(m)}}(\\xi ){{(x-{{x}_{0}})}^{m}}\\]\rWwhere \\({{g}^{(s)}}={{\\left. \\frac{{{\\partial }^{s}}g(x)}{\\partial {{x}^{2}}} \\right|}_{x={{x}_{0}}}}\\) and and \\(\\xi\\) lies between \\(x\\) and \\({{x}_{0}}\\)\nMSE, variance and biases\rSay \\(\\hat{\\theta }\\) is an estimator for true \\(\\theta\\), then the Mean Squared Error \\(MSE\\) of an estimator \\(\\hat{\\theta }\\) is the mean of squared deviation between \\(\\hat{\\theta }\\) and \\(\\theta\\) and given as:\r\\[MSE=E\\left[ {{\\left( \\hat{\\theta }-\\theta \\right)}^{2}} \\right] \\]\r\\[MSE=E\\left[ \\left( {{{\\hat{\\theta }}}^{2}}-2\\hat{\\theta }\\theta +{{\\theta }^{2}} \\right) \\right] \\]\r\\[MSE=E[{{\\hat{\\theta }}^{2}}]+E[{{\\theta }^{2}}]-2\\theta E[\\hat{\\theta }]\\]\r\\[MSE=\\underbrace{E[{{{\\hat{\\theta }}}^{2}}]-{{E}^{2}}[{{{\\hat{\\theta }}}^{2}}]}_{\\operatorname{var}(\\hat{\\theta })}+\\underbrace{{{E}^{2}}[{{{\\hat{\\theta }}}^{2}}]+E[{{\\theta }^{2}}]-2\\theta E[\\hat{\\theta }]}_{E{{\\left( E[\\hat{\\theta }]-\\theta \\right)}^{2}}}\\]\r\\[MSE=\\operatorname{var}(\\hat{\\theta })+\\underbrace{E{{\\left( E[\\hat{\\theta }]-\\theta \\right)}^{2}}}_{squared\\ of\\ bias\\ of\\ \\hat{\\theta }}\\]\r\\[MSE=\\operatorname{var}(\\hat{\\theta })+{{\\left\\{ bias(\\hat{\\theta }) \\right\\}}^{2}}\\]\nNote: \\(\\underbrace{E{{\\left( \\hat{\\theta }-E[\\hat{\\theta }] \\right)}^{2}}}_{\\operatorname{var}(\\hat{\\theta })}=\\underbrace{E[{{{\\hat{\\theta }}}^{2}}]-{{E}^{2}}[{{{\\hat{\\theta }}}^{2}}]}_{\\operatorname{var}(\\hat{\\theta })}\\).\nAnother way of solution is given as :\r\\[MSE=E\\left[ {{\\left( \\hat{\\theta }-\\theta \\right)}^{2}} \\right] \\]\r\\[MSE=E\\left[ {{\\left( \\hat{\\theta }-E[\\hat{\\theta }]+E[\\hat{\\theta }]-\\theta \\right)}^{2}} \\right]\\]\r\\[MSE=E\\left[ {{\\left( \\hat{\\theta }-E[\\hat{\\theta }] \\right)}^{2}}+{{\\left( E[\\hat{\\theta }]-\\theta \\right)}^{2}}+2\\left( \\hat{\\theta }-E[\\hat{\\theta }] \\right)\\left( E[\\hat{\\theta }]-\\theta \\right) \\right]\\]\r\\[MSE=\\underbrace{E{{\\left( \\hat{\\theta }-E[\\hat{\\theta }] \\right)}^{2}}}_{\\operatorname{var}(\\hat{\\theta })}+\\underbrace{E{{\\left( E[\\hat{\\theta }]-\\theta \\right)}^{2}}}_{squared\\ of\\ bias\\ of\\hat{\\theta }}+2E\\left[ \\left( \\hat{\\theta }-E[\\hat{\\theta }] \\right)\\left( E[\\hat{\\theta }]-\\theta \\right) \\right]\\]\r\\[MSE=\\operatorname{var}(\\hat{\\theta })+{{\\left\\{ bias(\\hat{\\theta }) \\right\\}}^{2}}+2E\\left[ \\left( \\hat{\\theta }E[\\hat{\\theta }]+\\hat{\\theta }\\theta -E[\\hat{\\theta }]E[\\hat{\\theta }]-E[\\hat{\\theta }]\\theta \\right) \\right]\\]\r\\[MSE=\\operatorname{var}(\\hat{\\theta })+{{\\left\\{ bias(\\hat{\\theta }) \\right\\}}^{2}}+2E\\left[ \\left( \\underbrace{\\hat{\\theta }E[\\hat{\\theta }]-E[\\hat{\\theta }]E[\\hat{\\theta }]}_{0}+\\underbrace{\\hat{\\theta }\\theta -E[\\hat{\\theta }]\\theta }_{0} \\right) \\right]\\]\r\\[MSE=\\operatorname{var}(\\hat{\\theta })+{{\\left\\{ bias(\\hat{\\theta }) \\right\\}}^{2}}\\]\nTheorem 1.1.\rLet \\({{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}}\\) \\(i.i.d\\) observation having a three-times differentiable \\(PDF\\) \\(f(x)\\), and \\({{f}^{s}}(x)\\) denote the \\(s-th\\) order derivative of \\(f(x)\\ s=(1,2,3)\\). Let \\(x\\) be an interior point in the support of \\(X\\), and let \\(\\hat{f}(x)\\) be \\(\\frac{1}{2h}{{n}^{-1}}\\left\\{ \\#\\ of\\ {{X}_{i}}\\ in\\ between\\ \\left[ x-h,x+h \\right] \\right\\}\\). Assume that the kernel function \\(k\\left( \\centerdot \\right)\\) bounded and satisfies: \\(\\int{k(v)dv=1}\\), \\(k(v)=k(-v)\\) and \\(\\int{{{v}^{2}}k(v)dv={{\\kappa }_{2}}\u0026gt;0}\\). And as \\(n\\to\\infty\\), \\(h\\to 0\\) and \\(nh\\to\\infty\\) then, the \\(MSE\\) of estimator \\(\\hat{f}(x)\\) is given as:\r\\[MSE\\left( \\hat{f}(x) \\right)=\\frac{{{h}^{4}}}{4}{{\\left[ {{\\kappa }_{2}}{{f}^{\\left( 2 \\right)}}(x) \\right]}^{2}}+\\frac{\\kappa f(x)}{nh}+o\\left( {{h}^{4}}+{{(nh)}^{-1}} \\right)=O\\left( {{h}^{4}}+{{(nh)}^{-1}} \\right)\\]\rWhere, \\(v=\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\), \\({{\\kappa }_{2}}=\\int{{{v}^{2}}k(v)dv}\\) and \\(\\kappa =\\int{{{k}^{2}}(v)dv}\\)\nProof of Theorem 1.1\rMSE, variance and biases\rWe can express the relationship of MSE, variance and bias of estimator \\(MSE\\left( \\hat{f}(x) \\right)\\) as:\n\\[MSE\\left( \\hat{f}(x) \\right)=\\operatorname{var}\\left( \\hat{f}(x) \\right)+{{\\left\\{ bias\\left( \\hat{f}(x) \\right) \\right\\}}^{2}}\\]\nThen we deal with \\(\\left\\{ bias\\left( \\hat{f}(x) \\right) \\right\\}\\) and \\(\\operatorname{var}\\left( \\hat{f}(x) \\right)\\) separately.\nBiases\rThe bias of \\(\\hat{f}(x)\\) is given as\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}=E[\\hat{f}(x)]-f(x)\\]\n\\[bias\\left\\{ \\hat{f}(x) \\right\\}=E\\left[ \\frac{1}{nh}\\sum\\limits_{i=1}^{n}{k\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right]-f(x)\\]\rBy the identical distribution, we can write:\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{nh}nE\\left[ k\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right]-f(x)\\]\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}={{h}^{-1}}\\int{f({{x}_{1}})}k\\left( \\frac{{{x}_{1}}-x}{h} \\right)d{{x}_{1}}-f(x)\\]\rNote: \\(\\frac{{{x}_{1}}-x}{h}=v\\); \\({{x}_{1}}-x=hv\\); \\({{x}_{1}}=x+hv\\); \\(\\frac{d{{x}_{1}}}{dv}=\\frac{d}{dv}\\left( x+hv \\right)=h\\) and \\(d{{x}_{1}}=hdv\\).\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}={{h}^{-1}}\\int{f(x+hv)}k\\left( v \\right)hdv-f(x)\\]\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}={{h}^{-1}}h\\int{f(x+hv)}k\\left( v \\right)dv-f(x)\\]\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\int{f(x+hv)}k\\left( v \\right)dv-f(x)\\]\rLet’s expand \\(f(x+hv)\\) with Taylor series expansion evaluated at \\(x\\). Since \\(f(x)\\) is only three times differentiable: \\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\int{\\left\\{ f(x)+{{f}^{(1)}}(x)(x+hv-x)+\\frac{1}{2!}{{f}^{(2)}}(x){{(x+hv-x)}^{2}}+\\frac{1}{3!}{{f}^{(3)}}(\\tilde{x}){{(x+hv-x)}^{3}} \\right\\}}k\\left( v \\right)dv-f(x)\\]\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\int{\\left\\{ f(x)+{{f}^{(1)}}(x)hv+\\frac{1}{2!}{{f}^{(2)}}(x){{h}^{2}}{{v}^{2}}+\\frac{1}{3!}{{f}^{(3)}}(\\tilde{x}){{h}^{3}}{{v}^{3}} \\right\\}}k\\left( v \\right)dv-f(x)\\]\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\int{\\left\\{ f(x)+{{f}^{(1)}}(x)hv+\\frac{1}{2!}{{f}^{(2)}}(x){{h}^{2}}{{v}^{2}}+O({{h}^{3}}) \\right\\}}k\\left( v \\right)dv-f(x)\\]\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}=f(x)\\int{k\\left( v \\right)dv}+{{f}^{(1)}}(x)h\\int{v}k\\left( v \\right)dv+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x)\\int{{{v}^{2}}}k\\left( v \\right)dv+\\int{O({{h}^{3}})k\\left( v \\right)dv}-f(x)\\]\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}=f(x)\\underbrace{\\int{k\\left( v \\right)dv}}_{1}+{{f}^{(1)}}(x)h\\int{v}k\\left( v \\right)dv+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x)\\underbrace{\\int{{{v}^{2}}}k\\left( v \\right)dv}_{{{\\kappa }_{2}}}+\\underbrace{\\int{O({{h}^{3}})k\\left( v \\right)dv}}_{O({{h}^{3}})}-f(x)\\]\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}=f(x)+{{f}^{(1)}}(x)h\\int{v}k\\left( v \\right)dv+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})-f(x)\\]\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}={{f}^{(1)}}(x)h\\int{v}k\\left( v \\right)dv+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})\\]\rBy symmetrical definition\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}={{f}^{(1)}}(x)h\\left[ \\left\\{ \\int\\limits_{-\\infty }^{0}{+\\int\\limits_{0}^{\\infty }{{}}} \\right\\}vk\\left( v \\right)dv \\right]+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})\\]\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}={{f}^{(1)}}(x)h\\left[ \\left\\{ \\int\\limits_{-\\infty }^{0}{vk\\left( v \\right)dv+\\int\\limits_{0}^{\\infty }{vk\\left( v \\right)dv}} \\right\\} \\right]+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})\\]\rThen, we can switch the bound of definite integrals. Click for tutorial.\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}={{f}^{(1)}}(x)h\\underbrace{\\left[ \\left\\{ -\\int\\limits_{0}^{\\infty }{vk\\left( v \\right)dv+\\int\\limits_{0}^{\\infty }{vk\\left( v \\right)dv}} \\right\\} \\right]}_{0}+\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})\\]\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+O({{h}^{3}})\\]\r\\[bias\\left\\{ \\hat{f}(x) \\right\\}=\\frac{{{h}^{2}}}{2!}{{f}^{(2)}}(x){{\\kappa }_{2}}+o({{h}^{2}})\\]\nVariance\rThe variance of \\(\\hat{f}(x)\\) is given as:\n\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=E{{\\left( \\hat{f}(x)-E\\left[ \\hat{f}(x) \\right] \\right)}^{2}}=E\\left[ {{\\left( \\hat{f}(x) \\right)}^{2}} \\right]-{{E}^{2}}\\left[ {{\\left( \\hat{f}(x) \\right)}^{2}} \\right]\\]\n\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\operatorname{var}\\left[ \\frac{1}{nh}\\sum\\limits_{i=1}^{n}{k\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right]\\]\nFor \\(b\\in \\mathbb{R}\\) be a constant and \\(y\\) be a random variable, then, \\(\\operatorname{var}[by]={{b}^{2}}\\operatorname{var}[y]\\), therefore,\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{{{n}^{2}}{{h}^{2}}}\\operatorname{var}\\left[ \\sum\\limits_{i=1}^{n}{k\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right]\\]\nFor \\(\\operatorname{var}(a+b)=\\operatorname{var}(a)+\\operatorname{var}(b)+2\\operatorname{cov}(a,b)\\) and if \\(a\\bot b\\) then \\(2\\operatorname{cov}(a,b)=0\\). In above expression \\({{X}_{i}}\\) are independent observation therefore \\({{X}_{i}}\\bot {{X}_{j}}\\ \\forall \\ i\\ne j\\). Hence, we can express\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{{{n}^{2}}{{h}^{2}}}\\left\\{ \\sum\\limits_{i=1}^{n}{\\operatorname{var}\\left[ k\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]}+0 \\right\\}\\]\rNote, \\({{X}_{i}}\\) are also identical, therefore \\(\\operatorname{var}({{X}_{i}})=\\operatorname{var}({{X}_{j}})\\) so, \\(\\sum\\limits_{i=1}^{n}{\\operatorname{var}({{X}_{i}})}=n\\operatorname{var}({{X}_{1}})\\). Therefore,\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{{{n}^{2}}{{h}^{2}}}n\\operatorname{var}\\left[ k\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right]\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\operatorname{var}\\left[ k\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right]\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ E\\left[ {{k}^{2}}\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right]-\\left[ E{{\\left( k\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right)}^{2}} \\right] \\right\\}\\]\rWhich is equivalent to:\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ \\int{f({{x}_{1}}){{k}^{2}}\\left( \\frac{{{x}_{1}}-x}{h} \\right)d{{x}_{1}}}-{{\\left[ \\int{f({{x}_{1}})k\\left( \\frac{{{x}_{1}}-x}{h} \\right)d{{x}_{1}}} \\right]}^{2}} \\right\\}\\]\rNote: \\(\\frac{{{x}_{1}}-x}{h}=v\\); \\({{x}_{1}}-x=hv\\); \\({{x}_{1}}=x+hv\\); \\(\\frac{d{{x}_{1}}}{dv}=\\frac{d}{dv}\\left( x+hv \\right)=h\\) and \\(d{{x}_{1}}=hdv\\).\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ \\int{f(x+hv){{k}^{2}}\\left( v \\right)hdv}-{{\\left[ \\int{f(x+hv)k\\left( v \\right)hdv} \\right]}^{2}} \\right\\}\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ h\\int{f(x+hv){{k}^{2}}\\left( v \\right)dv}-{{\\left[ h\\int{f(x+hv)k\\left( v \\right)dv} \\right]}^{2}} \\right\\}\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ \\int{f(x+hv){{k}^{2}}\\left( v \\right)hdv}-\\underbrace{{{\\left[ \\int{f(x+hv)k\\left( v \\right)hdv} \\right]}^{2}}}_{O\\left( {{h}^{2}} \\right)} \\right\\}\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ \\int{f(x+hv){{k}^{2}}\\left( v \\right)hdv}-O\\left( {{h}^{2}} \\right) \\right\\}\\]\rTaylor series expansion\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ h\\int{f(x)+{{f}^{(1)}}(\\xi )(x+hv-x){{k}^{2}}\\left( v \\right)dv}-O\\left( {{h}^{2}} \\right) \\right\\}\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ h\\int{f(x){{k}^{2}}\\left( v \\right)dv}+\\int{{{f}^{(1)}}(\\xi )(hv){{k}^{2}}\\left( v \\right)dv}-O\\left( {{h}^{2}} \\right) \\right\\}\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ hf(x)\\underbrace{\\int{{{k}^{2}}\\left( v \\right)dv}}_{\\kappa }+\\underbrace{\\int{{{f}^{(1)}}(\\xi )(hv){{k}^{2}}\\left( v \\right)dv}}_{O\\left( {{h}^{2}} \\right)}-O\\left( {{h}^{2}} \\right) \\right\\}\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{n{{h}^{2}}}\\left\\{ h\\kappa f(x)+O\\left( {{h}^{2}} \\right) \\right\\}\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=\\frac{1}{nh}\\left\\{ \\kappa f(x)+O\\left( h \\right) \\right\\}=O\\left( {{(nh)}^{-1}} \\right)\\]\nWe now know that the order of variance is \\(O\\left( {{(nh)}^{-1}} \\right)\\), the order of bias is \\(O\\left( {{h}^{2}} \\right)\\) and the order of biases square is \\(O\\left( {{h}^{4}} \\right)\\). As we know the the MSE is sum of variance and square of biases and pluggin the values of variance and bias, we get,\n\\[MSE\\left( \\hat{f}(x) \\right)=\\operatorname{var}\\left( \\hat{f}(x) \\right)+{{\\left\\{ bias\\left( \\hat{f}(x) \\right) \\right\\}}^{2}}\\]\n\\[MSE\\left( \\hat{f}(x) \\right)=\\frac{{{h}^{4}}}{4}{{\\left[ {{\\kappa }_{2}}{{f}^{\\left( 2 \\right)}}(x) \\right]}^{2}}+\\frac{\\kappa f(x)}{nh}+o\\left( {{h}^{4}}+{{(nh)}^{-1}} \\right)=O\\left( {{h}^{4}}+{{(nh)}^{-1}} \\right)\\]\rWhere, \\(v=\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\), \\({{\\kappa }_{2}}=\\int{{{v}^{2}}k(v)dv}\\) and \\(\\kappa =\\int{{{k}^{2}}(v)dv}\\)\nMultivariate NonParametric Density Estimation\rTheorem\rSuppose that \\({{X}_{1}},...,{{X}_{n}}\\) constitute an i.i.d \\(q\\)-vector \\(\\left( {{X}_{i}}\\in {{\\mathbb{R}}^{q}} \\right)\\) for some \\(q\u0026gt;1\\) having common PDF \\(f(x)=f({{x}_{1}},{{x}_{2}},...,{{x}_{q}})\\). Let \\({{X}_{ij}}\\) denote the \\(j-th\\) component of \\({{X}_{i}}(j=1,...,q)\\). Then the estimated pdf given by \\(\\hat{f}(x)\\)is constructed by product kernel function or the product of univariate kernel functions.\n\\[\\hat{f}(x)={{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{k\\left( \\frac{{{x}_{i1}}-{{x}_{1}}}{{{h}_{1}}} \\right)\\times k\\left( \\frac{{{x}_{i2}}-{{x}_{2}}}{{{h}_{2}}} \\right)}\\times \\cdots \\times k\\left( \\frac{{{x}_{iq}}-{{x}_{q}}}{{{h}_{q}}} \\right)\\]\r\\[\\hat{f}(x)={{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{\\prod\\limits_{j=1}^{q}{k\\left( \\frac{{{x}_{ij}}-{{x}_{j}}}{{{h}_{j}}} \\right)}}\\]\nBias term\rThe MSE consistency of \\(\\hat{f}(x)\\)is sum of variance and square of bias term. First, we define bias given as:\r\\[bias\\left( \\hat{f}(x) \\right)=E\\left( \\hat{f}(x) \\right)-f(x)\\]\r\\[bias\\left( \\hat{f}(x) \\right)=E\\left( {{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{\\prod\\limits_{j=1}^{q}{k\\left( \\frac{{{x}_{ij}}-{{x}_{j}}}{{{h}_{j}}} \\right)}} \\right)-f(x)\\]\rLets define \\(\\prod\\limits_{j=1}^{q}{k\\left( \\frac{{{X}_{ij}}-{{x}_{i}}}{{{h}_{j}}} \\right)}=K\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\)\r\\[bias\\left( \\hat{f}(x) \\right)=E\\left( {{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{K\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right)-f(x)\\]\r\\[bias\\left( \\hat{f}(x) \\right)={{(n{{h}_{1}}...{{h}_{q}})}^{-1}}E\\left( \\sum\\limits_{i=1}^{n}{K\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right)-f(x)\\]\r\\[bias\\left( \\hat{f}(x) \\right)={{(n{{h}_{1}}...{{h}_{q}})}^{-1}}n\\left( E\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right] \\right)-f(x)\\]\r\\[bias\\left( \\hat{f}(x) \\right)={{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left( E\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right] \\right)-f(x)\\]\r\\[bias\\left( \\hat{f}(x) \\right)={{({{h}_{1}}...{{h}_{q}})}^{-1}}\\int{K\\left( \\frac{{{X}_{i}}-x}{h} \\right)f\\left( {{X}_{i}} \\right)d{{x}_{i}}}-f(x)\\]\rNote: \\(d{{x}_{i}}\\) is vector comprise of \\(d{{x}_{i1}},d{{x}_{i2}},...,d{{x}_{iq}}\\) and\rlet’s \\(\\left( \\frac{{{X}_{i}}-x}{h} \\right)=\\left( \\frac{{{X}_{i1}}-{{x}_{1}}}{{{h}_{1}}},\\frac{{{X}_{i1}}-{{x}_{2}}}{{{h}_{2}}},...,\\frac{{{X}_{iq}}-{{x}_{q}}}{{{h}_{q}}} \\right)=\\psi =\\left( {{\\psi }_{1}},{{\\psi }_{2}},...,{{\\psi }_{q}} \\right)\\). This means, \\(\\frac{{{x}_{ij}}-{{x}_{j}}}{{{h}_{j}}}={{\\psi }_{j}}\\) and \\({{x}_{ij}}={{x}_{j}}+{{\\psi }_{j}}{{h}_{j}}\\) then \\(d{{x}_{ij}}={{h}_{j}}d{{\\psi }_{j}}\\)\r\\[bias\\left( \\hat{f}(x) \\right)={{({{h}_{1}}...{{h}_{q}})}^{-1}}\\int{K\\underbrace{\\left( \\frac{{{X}_{i}}-x}{h} \\right)}_{\\psi }\\underbrace{f\\left( {{X}_{i}} \\right)}_{f\\left( x+h\\psi \\right)}\\underbrace{d{{x}_{i}}}_{{{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }}-f(x)\\]\r\\[bias\\left( \\hat{f}(x) \\right)={{({{h}_{1}}...{{h}_{q}})}^{-1}}\\int{K\\left( \\psi \\right)f\\left( x+h\\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }-f(x)\\]\r\\[bias\\left( \\hat{f}(x) \\right)=({{h}_{1}}{{h}_{2}}...{{h}_{q}}){{({{h}_{1}}...{{h}_{q}})}^{-1}}\\int{K\\left( \\psi \\right)f\\left( x+h\\psi \\right)d\\psi }-f(x)\\]\r\\[bias\\left( \\hat{f}(x) \\right)=\\int{K\\left( \\psi \\right)f\\left( x+h\\psi \\right)d\\psi }-f(x)\\]\rNow perform a multivariate Taylor expansion for:\r\\[f\\left( x+h\\psi \\right)=\\left\\{ f(x)+{{f}^{1}}{{(x)}^{T}}\\left( x+h\\psi -x \\right)+\\frac{1}{2!}{{(x+h\\psi -x)}^{T}}{{f}^{2}}(x)\\left( x+h\\psi -x \\right)+\\frac{1}{3!}\\sum\\limits_{|l|=3}{{{D}_{l}}f\\left( {\\tilde{x}} \\right){{\\left( x+h\\psi -x \\right)}^{3}}} \\right\\}\\]\r\\[f\\left( x+h\\psi \\right)=\\left\\{ f(x)+{{f}^{1}}{{(x)}^{T}}\\left( h\\psi \\right)+\\frac{1}{2!}{{(h\\psi )}^{T}}{{f}^{2}}(x)\\left( h\\psi \\right)+\\frac{1}{3!}\\sum\\limits_{|l|=3}{{{D}_{l}}f\\left( {\\tilde{x}} \\right){{\\left( h\\psi \\right)}^{3}}} \\right\\}\\]\r\\[bias\\left( \\hat{f}(x) \\right)=\\int{K\\left( \\psi \\right)\\left\\{ f(x)+{{f}^{1}}{{(x)}^{T}}\\left( h\\psi \\right)+\\frac{1}{2!}{{(h\\psi )}^{T}}{{f}^{2}}(x)\\left( h\\psi \\right)+\\frac{1}{3!}\\sum\\limits_{|l|=3}{{{D}_{l}}f\\left( {\\tilde{x}} \\right){{\\left( h\\psi \\right)}^{3}}} \\right\\}d\\psi }-f(x)\\]\r\\[\\begin{matrix}\rbias\\left( \\hat{f}(x) \\right)= \u0026amp; \\int{f(x)K\\left( \\psi \\right)d\\psi }+\\int{{{f}^{1}}{{(x)}^{T}}\\left( h\\psi \\right)K\\left( \\psi \\right)d\\psi }+\\int{\\frac{1}{2!}{{(h\\psi )}^{T}}{{f}^{2}}(x)\\left( h\\psi \\right)K\\left( \\psi \\right)d\\psi } \\\\\r{} \u0026amp; +\\int{\\frac{1}{3!}\\sum\\limits_{|l|=3}{{{D}_{l}}f\\left( {\\tilde{x}} \\right){{\\left( h\\psi \\right)}^{3}}K\\left( \\psi \\right)d\\psi }-f(x)} \\\\\r\\end{matrix}\\]\r\\[bias\\left( \\hat{f}(x) \\right)=f\\left( x \\right)\\int{K\\left( \\psi \\right)d\\psi }+{{f}^{1}}{{(x)}^{T}}h\\int{\\psi K\\left( \\psi \\right)d\\psi }+\\int{\\frac{1}{2!}{{(h\\psi )}^{T}}{{f}^{2}}(x)\\left( h\\psi \\right)K\\left( \\psi \\right)d\\psi +O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)-f\\left( x \\right)}\\]\r\\[bias\\left( \\hat{f}(x) \\right)=f\\left( x \\right)\\underbrace{\\int{K\\left( \\psi \\right)d\\psi }}_{1}+{{f}^{1}}{{(x)}^{T}}h\\underbrace{\\int{\\psi K\\left( \\psi \\right)d\\psi }}_{0}+\\frac{{{h}^{T}}h}{2}\\int{K\\left( \\psi \\right){{\\psi }^{T}}{{f}^{2}}(x)\\psi d\\psi +O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)-f\\left( x \\right)}\\]\r\\[bias\\left( \\hat{f}(x) \\right)=\\frac{1}{2}\\int{{{h}^{T}}h{{\\psi }^{T}}{{f}^{2}}(x)\\psi K\\left( \\psi \\right)d\\psi }+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)\\]\r\\[bias\\left( \\hat{f}(x) \\right)=\\frac{1}{2}\\int{\\underbrace{{{h}^{T}}h\\underbrace{{{\\psi }^{T}}}_{1\\times q}\\underbrace{{{f}^{2}}(x)}_{q\\times q}\\underbrace{\\psi }_{q\\times 1}}_{1\\times 1}K\\left( \\psi \\right)d\\psi }+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)\\]\r\\[bias\\left( \\hat{f}(x) \\right)=\\frac{1}{2}\\int{{{h}^{T}}h\\sum\\limits_{l=1}^{q}{\\sum\\limits_{m=1}^{q}{f_{lm}^{\\left( 2 \\right)}\\left( x \\right){{\\psi }_{l}}{{\\psi }_{m}}}}K\\left( \\psi \\right)d\\psi }+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)\\]\rFor \\(l\\ne m\\) the cross derivatives will be zero hence, we can re-write as:\r\\[bias\\left( \\hat{f}(x) \\right)=\\frac{1}{2}\\sum\\limits_{l=1}^{q}{h_{l}^{2}f_{ll}^{\\left( 2 \\right)}\\left( x \\right){{\\kappa }_{2}}}+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)\\]\r\\[bias\\left( \\hat{f}(x) \\right)=\\frac{{{\\kappa }_{2}}}{2}\\sum\\limits_{l=1}^{q}{h_{l}^{2}f_{ll}^{\\left( 2 \\right)}\\left( x \\right)}+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)=O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right)\\]\r\\[bias\\left( \\hat{f}(x) \\right)=\\underbrace{\\frac{{{\\kappa }_{2}}}{2}\\sum\\limits_{l=1}^{q}{h_{l}^{2}f_{ll}^{\\left( 2 \\right)}\\left( x \\right)}}_{{{c}_{1}}}+O\\left( \\sum\\limits_{j=1}^{q}{h_{j}^{3}} \\right)=O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right)\\]\nWhere, \\({{c}_{1}}\\) is a constant.\nVariance term\rThe variance is given as:\r\\[\\operatorname{var}\\left( \\hat{f}(x) \\right)=E\\left( {{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{\\prod\\limits_{j=1}^{q}{k\\left( \\frac{{{x}_{ij}}-{{x}_{j}}}{{{h}_{j}}} \\right)}} \\right)\\]\rLets’s define \\(\\prod\\limits_{j=1}^{q}{k\\left( \\frac{{{X}_{ij}}-{{x}_{i}}}{{{h}_{j}}} \\right)}=K\\left( \\frac{{{X}_{i}}-x}{h} \\right)\\)\r\\[\\operatorname{var}\\left( \\hat{f}(x) \\right)=\\operatorname{var}\\left( {{(n{{h}_{1}}...{{h}_{q}})}^{-1}}\\sum\\limits_{i=1}^{n}{K\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right)\\]\rFor \\(b\\in \\mathbb{R}\\) be a constant and \\(y\\) be a random variable, then, \\(\\operatorname{var}[by]={{b}^{2}}\\operatorname{var}[y]\\), therefore,\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}={{(n{{h}_{1}}...{{h}_{q}})}^{-2}}\\operatorname{var}\\left[ \\sum\\limits_{i=1}^{n}{K\\left( \\frac{{{X}_{i}}-x}{h} \\right)} \\right]\\]\nFor \\(\\operatorname{var}(a+b)=\\operatorname{var}(a)+\\operatorname{var}(b)+2\\operatorname{cov}(a,b)\\) and if \\(a\\bot b\\) then \\(2\\operatorname{cov}(a,b)=0\\). In above expression \\({{X}_{i}}\\) are independent observation therefore \\({{X}_{i}}\\bot {{X}_{j}}\\ \\forall \\ i\\ne j\\). Hence, we can express\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}={{(n{{h}_{1}}...{{h}_{q}})}^{-2}}\\left\\{ \\sum\\limits_{i=1}^{n}{\\operatorname{var}\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]}+0 \\right\\}\\]\rNote, \\({{X}_{i}}\\) are also identical, therefore \\(\\operatorname{var}({{X}_{i}})=\\operatorname{var}({{X}_{j}})\\) so, \\(\\sum\\limits_{i=1}^{n}{\\operatorname{var}({{X}_{i}})}=n\\operatorname{var}({{X}_{1}})\\). Therefore,\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}={{(n{{h}_{1}}...{{h}_{q}})}^{-2}}n\\operatorname{var}\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\operatorname{var}\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]\\]\rThe variance is given as: \\(\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=E{{\\left( \\hat{f}(x)-E\\left[ \\hat{f}(x) \\right] \\right)}^{2}}=E\\left[ {{\\left( \\hat{f}(x) \\right)}^{2}} \\right]-{{E}^{2}}\\left[ {{\\left( \\hat{f}(x) \\right)}^{2}} \\right]\\)\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ E{{\\left[ K\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right]}^{2}}-\\left[ E{{\\left( K\\left( \\frac{{{X}_{1}}-x}{h} \\right) \\right)}^{2}} \\right] \\right\\}\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\int{f\\left( {{X}_{i}} \\right)\\left[ {{K}^{2}}\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]d{{x}_{i}}}-{{\\left[ \\int{f\\left( {{X}_{i}} \\right)\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]d{{x}_{i}}} \\right]}^{2}}\\]\rNote: \\(d{{x}_{i}}\\) is vector comprise of \\(d{{x}_{i1}},d{{x}_{i2}},...,d{{x}_{iq}}\\) and\rlet’s \\(\\left( \\frac{{{X}_{i}}-x}{h} \\right)=\\left( \\frac{{{X}_{i1}}-{{x}_{1}}}{{{h}_{1}}},\\frac{{{X}_{i1}}-{{x}_{2}}}{{{h}_{2}}},...,\\frac{{{X}_{iq}}-{{x}_{q}}}{{{h}_{q}}} \\right)=\\psi =\\left( {{\\psi }_{1}},{{\\psi }_{2}},...,{{\\psi }_{q}} \\right)\\). This means, \\(\\frac{{{x}_{ij}}-{{x}_{j}}}{{{h}_{j}}}={{\\psi }_{j}}\\) and \\({{x}_{ij}}={{x}_{j}}+{{\\psi }_{j}}{{h}_{j}}\\) then \\(d{{x}_{ij}}={{h}_{j}}d{{\\psi }_{j}}\\)\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\int{\\left[ {{K}^{2}}\\underbrace{\\left( \\frac{{{X}_{i}}-x}{h} \\right)}_{\\psi } \\right]\\underbrace{f\\left( {{X}_{i}} \\right)}_{f\\left( x+h\\psi \\right)}\\underbrace{d{{x}_{i}}}_{{{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }}-{{\\left[ \\int{f\\left( {{X}_{i}} \\right)\\left[ K\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right]d{{x}_{i}}} \\right]}^{2}} \\right\\}\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\int{{{K}^{2}}\\left( \\psi \\right)f\\left( x+h\\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }-{{\\left[ \\int{f\\left( x+h\\psi \\right)K\\left( \\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi } \\right]}^{2}} \\right\\}\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\int{{{K}^{2}}\\left( \\psi \\right)f\\left( x+h\\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }-\\underbrace{{{\\left[ {{({{h}_{1}}{{h}_{2}}...{{h}_{q}})}^{2}}\\int{f\\left( x+h\\psi \\right)K\\left( \\psi \\right)d\\psi } \\right]}^{2}}}_{O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right)} \\right\\}\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\int{{{K}^{2}}\\left( \\psi \\right)f\\left( x+h\\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }-O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ {{h}_{1}}{{h}_{2}}...{{h}_{q}}\\int{{{K}^{2}}\\left( \\psi \\right)f\\left( x+h\\psi \\right)d\\psi }-O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}\\]\rNow, we perform the Taylor series expansion\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\int{f\\left( x \\right){{K}^{2}}\\left( \\psi \\right){{h}_{1}}{{h}_{2}}...{{h}_{q}}d\\psi }+\\int{{{f}^{(1)}}\\left( \\xi \\right)\\left( {{h}_{1}}{{h}_{2}}...{{h}_{q}}\\psi \\right){{K}^{2}}\\left( \\psi \\right)d\\psi }-O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\left( {{h}_{1}}{{h}_{2}}...{{h}_{q}} \\right)f\\left( x \\right)\\underbrace{\\int{{{K}^{2}}\\left( \\psi \\right)d\\psi }}_{\\mathbf{\\kappa }}+\\underbrace{\\int{{{f}^{(1)}}\\left( \\xi \\right)\\left( {{h}_{1}}{{h}_{2}}...{{h}_{q}}\\psi \\right){{K}^{2}}\\left( \\psi \\right)d\\psi }}_{O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right)}-O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\left( {{h}_{1}}{{h}_{2}}...{{h}_{q}} \\right)f\\left( x \\right)\\mathbf{\\kappa }+O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}\\]\r\\[\\operatorname{var}\\left\\{ \\hat{f}(x) \\right\\}=n{{({{h}_{1}}...{{h}_{q}})}^{-1}}\\left\\{ \\left( {{h}_{1}}{{h}_{2}}...{{h}_{q}} \\right)f\\left( x \\right){{\\kappa }^{q}}+O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right\\}=O\\left( \\frac{1}{n{{h}_{1}}...{{h}_{q}}} \\right)\\]\nMSE term\rSummarizing, we obtain the MSE as:\r\\[MSE\\left( \\hat{f}\\left( x \\right) \\right)=\\operatorname{var}\\left( \\hat{f}\\left( x \\right) \\right)+{{\\left[ bias\\left( \\hat{f}\\left( x \\right) \\right) \\right]}^{2}}\\]\r\\[MSE\\left( \\hat{f}\\left( x \\right) \\right)=O\\left( \\frac{1}{n{{h}_{1}}...{{h}_{q}}} \\right)+{{\\left[ O\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right) \\right]}^{2}}=O\\left( {{\\left( \\sum\\limits_{l=1}^{q}{h_{l}^{2}} \\right)}^{2}}+{{\\left( n{{h}_{1}}{{h}_{2}}...{{h}_{q}} \\right)}^{-1}} \\right)\\]\nHence, if as \\(n\\to \\infty\\), \\({{\\max }_{1\\le l\\le q}}{{h}_{l}}\\to 0\\) and \\(n{{h}_{1}}...{{h}_{q}}\\to \\infty\\) then we have \\(\\hat{f}\\left( x \\right)\\to f\\left( x \\right)\\) in MSE, which \\(\\hat{f}\\left( x \\right)\\to f\\left( x \\right)\\) in probability.\nThe optimal band-width\r\\[MSE\\left( \\hat{f}\\left( x \\right) \\right)={{c}_{1}}{{h}^{4}}+{{c}_{2}}{{n}^{-1}}{{h}^{-q}}\\]\nNow, we can choose \\(h\\) to find the optimal value of above function and the \\(F.O.C\\) is given as:\n\\[\\frac{\\partial MSE\\left( \\hat{f}\\left( x \\right) \\right)}{\\partial h}=\\frac{{{c}_{1}}{{h}^{4}}+{{c}_{2}}{{n}^{-1}}{{h}^{-q}}}{\\partial h}=0\\]\n\\[4{{c}_{1}}{{h}^{3}}+{{c}_{2}}{{n}^{-1}}(-q){{h}^{-q-1}}=0\\]\n\\[\\frac{4{{c}_{1}}}{{{c}_{2}}q}n={{h}^{-q-1-3}}\\]\nLet’s define \\(\\frac{4{{c}_{1}}}{{{c}_{2}}q}=c\\) then\n\\[{{h}^{-q-4}}=cn\\]\n\\[{{h}^{*}}={{\\left[ cn \\right]}^{-\\frac{1}{4+q}}}\\]\nAsymptotic Normality of Density Estimators\rTheorem 1.2\nLet \\({{X}_{1}},...,{{X}_{n}}\\) be \\(i.i.d\\) \\(q-\\)vector with it’s \\(PDF\\) \\(f\\left( \\cdot \\right)\\) having three-times bounded continuous derivatives. Let \\(x\\) be an interior point of the support \\(X\\). If, as \\(n\\to \\infty\\), \\({{h}_{s}}\\to 0\\) for all \\(s=1,...,q\\), \\(n{{h}_{1}}n{{h}_{2}}...n{{h}_{q}}\\to \\infty\\) and \\(\\left( n{{h}_{1}}n{{h}_{2}}...n{{h}_{q}} \\right)\\sum\\nolimits_{s=1}^{q}{h_{s}^{6}}\\to 0\\), then,\r\\[\\sqrt{n{{h}_{1}}n{{h}_{2}}...n{{h}_{q}}}\\left[ \\hat{f}\\left( x \\right)-f\\left( x \\right)-\\frac{{{\\kappa }_{2}}}{2}\\sum\\limits_{s=1}^{q}{h_{s}^{2}{{f}_{ss}}\\left( x \\right)} \\right]\\overset{d}{\\mathop{\\to }}\\,N\\left( 0,{{\\kappa }^{q}}f\\left( x \\right) \\right)\\]\nBefore, we begin, imagine that each band width is equal i.e. \\({{h}_{1}}={{h}_{2}}=...={{h}_{q}}\\). Let’s acknowledge that as \\(n\\to \\infty\\), \\(h\\to 0\\). If this is the case then, where does \\(n{{h}^{q}}\\) go? Does this go to zero or infinity?\r\\({{h}^{q}}\\) doesn’t converge to zero as fast as \\(n\\to \\infty\\), then \\(n{{h}^{q}}\\to \\infty\\) but \\(n{{h}^{6+q}}\\to 0\\). Therefore, because of this reason we can set up asymptotic normality.\nLet begin with following equation:\r\\[\\sqrt{n{{h}^{q}}}\\left[ \\hat{f}\\left( x \\right)-E\\left[ \\hat{f}\\left( x \\right) \\right] \\right]\\]\nThis can be re-expressed as:\r\\[\\sqrt{n{{h}^{q}}}\\frac{1}{n{{h}^{q}}}\\sum\\limits_{i=1}^{n}{\\left[ k\\left( \\frac{{{X}_{i}}-x}{h} \\right)-E\\left[ k\\left( \\frac{{{X}_{i}}-x}{h} \\right) \\right] \\right]}\\]\rLet’s define\r\\[k\\left( \\frac{{{X}_{i}}-x}{h} \\right)={{K}_{i}}\\]\rthen,\r\\[\\frac{1}{\\sqrt{n{{h}^{q}}}}\\sum\\limits_{i=1}^{n}{\\left( {{K}_{i}}-E{{K}_{i}} \\right)}=\\sum\\limits_{i=1}^{n}{\\frac{1}{\\sqrt{n{{h}^{q}}}}\\left( {{K}_{i}}-E{{K}_{i}} \\right)}=\\sum\\limits_{i=1}^{n}{{{Z}_{n,i}}}\\]\rNote that the \\({{Z}_{n,i}}\\) is double array (or data frame, column indexed with different \\(n\\) and row with different observation \\(i\\), each combination of \\(n\\) and \\(i\\) gives different value for \\({{Z}_{n,i}}\\))\nNow, we need to think for a strategy for asymptotic normality. Actually, there are four ways (at least) to think about the asymptotic normality: 1) Khinchi’s Law of Large Numbers; 2) Lindeberg-Levy Central Limit Theorem; 3) Lindelberg-Feller Central Limit Theorem and 4) Lyapunov Central Limit Theorem. We will use Lyapunov Central Limit Theorem, for brief introduction to these laws of large numbers and central limit theorem, please see the annex.\nWe will consider following four conditions:\n\\(E\\left[ {{Z}_{ni}} \\right]=\\frac{1}{\\sqrt{n{{h}^{q}}}}\\left[ E{{K}_{i}}-E{{K}_{i}} \\right]=0\\)\n\\(Var\\left( {{Z}_{n,i}} \\right)=\\frac{1}{n{{h}^{q}}}Var\\left( {{K}_{i}} \\right)=\\frac{1}{n{{h}^{q}}}\\left[ EK_{i}^{2}-{{\\left( E{{K}_{i}} \\right)}^{2}} \\right]={{h}^{q}}\\frac{1}{n{{h}^{q}}}Var\\left( {{K}_{i}} \\right)={{h}^{q}}\\frac{{{\\kappa }^{q}}f\\left( x \\right)}{n{{h}^{q}}}\\left( 1+o\\left( 1 \\right) \\right)={{\\kappa }^{q}}f\\left( x \\right)\\left( 1+o\\left( 1 \\right) \\right)\\)\n\\(\\sigma _{n}^{2}=\\sum\\limits_{i=1}^{n}{Var\\left( {{Z}_{n,i}} \\right)}={{\\kappa }^{q}}f\\left( x \\right)\\left( 1+o\\left( 1 \\right) \\right)\\) and taking limits we get \\(\\underset{n\\to \\infty }{\\mathop{\\lim }}\\,\\sigma _{n}^{2}={{\\kappa }^{q}}f\\left( x \\right)\\)\n\\(\\sum\\limits_{i=1}^{n}{E{{\\left| {{Z}_{n,i}} \\right|}^{2+\\delta }}}=nE{{\\left| {{Z}_{n,i}} \\right|}^{2+\\delta }}\\) if each \\(i\\) are i.i.d\rWith Cram?r-Rao bound inequality we can write\r\\[n\\frac{1}{{{\\left| \\sqrt{n{{h}^{q}}} \\right|}^{2+\\delta }}}E{{\\left| {{K}_{i}}-E{{K}_{i}} \\right|}^{2+\\delta }}\\le \\frac{c}{{{n}^{{\\delta }/{2}\\;}}{{h}^{{\\delta }/{2}\\;}}}\\frac{1}{{{h}^{q}}}\\left[ E{{\\left| {{K}_{i}} \\right|}^{2+\\delta }}+{{\\left| E{{K}_{i}} \\right|}^{2+\\delta }} \\right]\\to 0\\]\rNote that \\(n{{h}^{q}}\\to \\infty\\) so \\(\\left( \\bullet \\right)\\to 0\\), further \\(E{{\\left| {{K}_{i}} \\right|}^{2+\\delta }}\\) is higher order than \\({{\\left| E{{K}_{i}} \\right|}^{2+\\delta }}\\)\rWith all these conditions:\r\\[\\sigma _{n}^{-2}\\frac{1}{\\sqrt{n{{h}^{q}}}}\\sum\\limits_{i=1}^{n}{\\left( {{K}_{i}}-E{{K}_{i}} \\right)}\\overset{d}{\\mathop{\\to }}\\,N\\left( 0,1 \\right)\\]\r\\[\\frac{1}{\\sqrt{n{{h}^{q}}}}\\sum\\limits_{i=1}^{n}{\\left( {{K}_{i}}-E{{K}_{i}} \\right)}\\overset{d}{\\mathop{\\to }}\\,N\\left( 0,\\underset{n\\to \\infty }{\\mathop{\\lim }}\\,\\sigma _{n}^{2} \\right)\\]\r\\[\\frac{1}{\\sqrt{n{{h}^{q}}}}\\sum\\limits_{i=1}^{n}{\\left( {{K}_{i}}-E{{K}_{i}} \\right)}\\overset{d}{\\mathop{\\to }}\\,N\\left( 0,{{\\kappa }^{q}}f\\left( x \\right) \\right)\\]\n","date":1546819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546819200,"objectID":"5df485a513dc3cfcae192c0b399ebd42","permalink":"https://ShishirShakya.github.io/post/2019-01-07-density-estimation/","publishdate":"2019-01-07T00:00:00Z","relpermalink":"/post/2019-01-07-density-estimation/","section":"post","summary":"Univariate Density Estimation\rParametric Desity Estimation\rDraw from a normal distribution\rGiven \\({{X}_{1}},{{X}_{2}},\\ldots ,{{X}_{n}}\\) \\(i.i.d\\) draw from a normal distribution with mean of \\(\\mu\\) and variance of \\({{\\sigma }^{2}}\\) the joint \\(PDF\\) can be expressed as:\r\\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\prod\\limits_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{i}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}}\\]\r\\[f\\left( {{X}_{1}},{{X}_{2}},\\ldots {{X}_{3}} \\right)=\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{1}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{2}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\times \\cdots \\times \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}{{e}^{-\\frac{{{\\left( {{X}_{n}}-\\mu \\right)}^{2}}}{2{{\\sigma }^{2}}}}}\\]\rThe term \\(\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}\\) is a constant multiplying this term for \\(n\\) times gives \\({{\\left( \\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}} \\right)}^{n}}=\\frac{1}{{{\\left( 2\\pi \\sigma \\right)}^{\\frac{n}{2}}}}\\).","tags":["Density,","R,","Non Parametric,","Theory,","Proof,"],"title":"Density Estimation","type":"post"},{"authors":null,"categories":["Causal Inference"],"content":"\rPotential Outcome Framework\rThere are at least three different school of thoughts regarding causality: 1) granger causality; 2) Rubin’s potential outcome framework and 3) Pearl’s causality. Each of these thoughts have their own pros and cons. I will quickly discuss about the Rubin’s potential outcome framework and show the fundamental problem of causal inference.\rFor an example, say a researcher wants to study impact of certain treatment (say performance enhancing drug say \\(T\\)) on some outcomes (say capacity to solve riddle quizzes say \\(Y\\)).\nHe randomly sampled the total population and divided the total sample population \\(N\\) to the control group of \\({{N}_{co}}\\) and treatment group \\({{N}_{Tr}}\\) such that \\({{N}_{co}}+{{N}_{Tr}}=N\\).\nSay the treatment group are exposed to the treatment i.e. \\(T=1\\) for intervening with performance enhancing drug and control group is not intervened or given a placebo i.e. \\(T=0\\).\nAfter this experiment, researcher takes test of all the individual and records numbers of minutes to solve the questions.\nThe average outcome of treated group is \\(E\\left[ Y\\left( 1 \\right)|T=1 \\right]\\) and average outcome of control group is given as \\(E\\left[ Y\\left( 0 \\right)|T=0 \\right]\\). Both averages can be estimated easily as the data are observed. The simple difference of means \\(SDoM\\) between average outcome of treated and average outcome of control group given as:\n\\[SDoM=E\\left[ Y\\left( 1 \\right)|T=1 \\right]-E\\left[ Y\\left( 0 \\right)|T=0 \\right]\\]\nThe potential outcome framework, however, look at this problem and ask a simple question i.e. “What is the counterfactual?”. In another word, what would be the outcome of those who are treated if they were not been treated i.e. \\(E\\left[ Y\\left( 0 \\right)|T=1 \\right]=??\\). Similarly, what would be the outcome of control if they had been treated i.e. \\(E\\left[ Y\\left( 1 \\right)|T=0 \\right]=??\\).\rThen, the average treatment effect is given as:\n\\[ATE=\\left\\{ E\\left[ Y\\left( 1 \\right)|T=1 \\right]-E\\left[ Y\\left( 0 \\right)|T=1 \\right] \\right\\}-\\left\\{ E\\left[ Y\\left( 0 \\right)|T=0 \\right]-E\\left[ Y\\left( 1 \\right)|T=0 \\right] \\right\\}\\]\nWhere,\n\\(E\\left[ Y\\left( 1 \\right)|T=1 \\right]\\) represents, given treated, the average outcome of treated group (observed in data).\r\\(E\\left[ Y\\left( 0 \\right)|T=1 \\right]\\) represents, given treated, the average outcome of treated group if they were controlled (unobserved).\r\\(E\\left[ Y\\left( 0 \\right)|T=0 \\right]\\) represents, given controlled, the average outcome of controlled group (observed in data).\r\\(E\\left[ Y\\left( 1 \\right)|T=0 \\right]\\) represents, given controlled, the average outcome of controlled group if they were treated (unobserved).\nWe can also correct this estimate with sampling weights as:\n\\[ATE=\\lambda \\left\\{ E\\left[ Y\\left( 1 \\right)|T=1 \\right]-E\\left[ Y\\left( 0 \\right)|T=1 \\right] \\right\\}-\\left( 1-\\lambda \\right)\\left\\{ E\\left[ Y\\left( 0 \\right)|T=0 \\right]-E\\left[ Y\\left( 1 \\right)|T=0 \\right] \\right\\}\\]\nWhere, \\(\\lambda =\\frac{{{N}_{Tr}}}{N}\\) or proportion of treated group and \\(1-\\lambda =1-\\frac{{{N}_{Tr}}}{N}=\\frac{{{N}_{co}}}{N}\\) proportion of control group.\nIf we closely look at estimate of \\(ATE\\) we find that each counterfactual of treated and control group is missing data problem, this is the fundamental problem of causal inference.\n.\n","date":1546819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546819200,"objectID":"f7fad2a171fac2421db9ece83ba65afa","permalink":"https://ShishirShakya.github.io/post/2018-12-01-rubins-potential-outcome-framework/","publishdate":"2019-01-07T00:00:00Z","relpermalink":"/post/2018-12-01-rubins-potential-outcome-framework/","section":"post","summary":"Potential Outcome Framework\rThere are at least three different school of thoughts regarding causality: 1) granger causality; 2) Rubin’s potential outcome framework and 3) Pearl’s causality. Each of these thoughts have their own pros and cons. I will quickly discuss about the Rubin’s potential outcome framework and show the fundamental problem of causal inference.\rFor an example, say a researcher wants to study impact of certain treatment (say performance enhancing drug say \\(T\\)) on some outcomes (say capacity to solve riddle quizzes say \\(Y\\)).","tags":["Causal Inference,","Theory,","Proof,"],"title":"Rubin's Potential Outcome Framework","type":"post"},{"authors":null,"categories":["R"],"content":"\rReplication can have a different meaning to the different discipline. Replication in studies that uses public data sources involve sharing codes to ensure consistency in the results. Here, I share some chunk of the codes in five steps using RStudio, that can ensure ease in shareability. These 5 steps are the preambles prior I proceed toward data management.\nFor this a user will require to install R here and R-Studio here. A copy of this code is available here.\nClean the current workspace and session.\rThe very first step in to remove all the objects and plots from the current workspace and device in the R. For this, I use following codes.\nrm(list = ls())\rdev.off(dev.list()[\u0026quot;RStudioGD\u0026quot;])\rInstall and load R packages\rSecond step is to install the required R-packages (if r-packages don’t exist installed package directory of R) and load them into the library. In the following code, I first create a function called load_packages(). This function takes list of names of packages as argument, then check if the packages are already installed in the user’s package list, and if not then installs the packages from the cran. I always use load_packages(\"rstudioapi\") package and it’s a mandatory package for mywork flow. Then I use load_packages() for other required packages. These packages depends upon the project. In the following example I use haven, and Hmisc packages.\nload_packages \u0026lt;- function(pkg){\rnew.pkg \u0026lt;- pkg[!(pkg %in% installed.packages()[, \u0026quot;Package\u0026quot;])]\rif (length(new.pkg)) install.packages(new.pkg, dependencies = TRUE,\rrepos = \u0026quot;http://cran.us.r-project.org\u0026quot;)\rsapply(pkg, require, character.only = TRUE)\r}\rload_packages(\u0026quot;rstudioapi\u0026quot;) # This is a mandatory package.\rload_packages (c(\u0026quot;devtools\u0026quot;, \u0026quot;haven\u0026quot;, \u0026quot;Hmisc\u0026quot;)) # These packages depends upon the project.\rSetup Working directory.\rSetting up the working directory is my third step. It is very important because my working directory can always be different then my coauthor’s/referee’s working directory. So, I first save the R script in the desired location manually and my coauthors are also advised to do similar. Then I use following code. This code detects where R-script is saved and sets that as working directory. Note this chunk of code uses rstudioapi package, therefore it’s a mandatory package for my workflow.\npath \u0026lt;- dirname(rstudioapi::getActiveDocumentContext()$path)\rsetwd(path)\rThe Folders\rI never right click and create new folders. I always code to create a folder. The code helps me to track what I did in logical manner. Following codes directly create the folders in the working directory. I usually like to have folder called rawdata to dump the data downloaded from the internet then I also like another folder outcomes to save my final dataset for analysis, plots and tables. Sometimes, I can create folders within folder like rawdata/beadataset.\ndir.create(file.path(path, \u0026quot;rawdata\u0026quot;))\rdir.create(file.path(path, \u0026quot;outcomes\u0026quot;))\rdir.create(file.path(path, \u0026quot;rawdata/beadataset\u0026quot;))\rGetting data.\rNever download the data manually. If possible, always provide a download link and use script to download the data. And never touch the data. It seems counter-intuitive but, I never open data in excel. If I open data in excel, I make sure I don’t save or If I have to play around with data, I do that in separate folder and delete them ASAP.\nConsider following example, the data of GDP by State by Year is available from the Bureau of Economic Analysis website. These data have stable link (the link doesn’t change over time) and content of data are consistent. The data can be download in the .zip format. Again, I write script to unzip the data. The script checks if folder called rawdata has gdpstate_naics_all_C.zip file or not. If there exist no file, the script will download the file.\ngdpbystatebyyear \u0026lt;- \u0026quot;https://www.bea.gov/regional/zip/gdpstate/gdpstate_naics_all_C.zip\u0026quot;\rif (file.exists(\u0026quot;rawdata/gdpstate_naics_all_C.zip\u0026quot;) == FALSE) { # get the zip file\rdownload.file(gdpbystatebyyear,\rdestfile = \u0026quot;rawdata/gdpstate_naics_all_C.zip\u0026quot;, mode=\u0026quot;wb\u0026quot;)\r}\runzip(\u0026quot;rawdata/gdpstate_naics_all_C.zip\u0026quot;, exdir = paste0(path,\u0026quot;/rawdata/beadataset\u0026quot;))\rConsider another example, if data is not available from in the web, I can share them via my google drive. I upload the zip file in google drive then get the public shareable link. The object gdrivepublic comprises of the public shareable link. Like previous chunk of code, here I check if data exist or not then download.\ngdrivepublic \u0026lt;- \u0026quot;https://drive.google.com/uc?authuser=0\u0026amp;id=1AiZda_1-2nwrxI8fLD0Y6e5rTg7aocv0\u0026amp;export=download\u0026quot;\rif (file.exists(\u0026quot;datafromGoogleDrive.zip\u0026quot;) == FALSE) { # get the zip file\rdownload.file(gsub(\u0026quot;open\\\\?\u0026quot;, \u0026quot;uc\\\\?export=download\\\\\u0026amp;\u0026quot;, gdrivepublic), destfile = \u0026quot;datafromGoogleDrive.zip\u0026quot;, mode=\u0026quot;wb\u0026quot;)\r}\r","date":1545782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545782400,"objectID":"ecb40a5a1bcfd5aecaaf9e7a172ac4f7","permalink":"https://ShishirShakya.github.io/post/2018-12-26-preamble-of-reproducible-research/","publishdate":"2018-12-26T00:00:00Z","relpermalink":"/post/2018-12-26-preamble-of-reproducible-research/","section":"post","summary":"Replication can have a different meaning to the different discipline. Replication in studies that uses public data sources involve sharing codes to ensure consistency in the results. Here, I share some chunk of the codes in five steps using RStudio, that can ensure ease in shareability. These 5 steps are the preambles prior I proceed toward data management.\nFor this a user will require to install R here and R-Studio here.","tags":["Reproducible,","R,"],"title":"Preambles for Reproducible Research","type":"post"},{"authors":null,"categories":["Theoritical Statistics"],"content":"\rTheorem 1: Gaussian Tail Inequality\rGiven \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)\\) then, \\(P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }\\) and \\(P\\left( \\left| {{{\\bar{X}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2}{\\sqrt{n}\\varepsilon }{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\overset{l\\arg e\\ n}{\\mathop{\\le }}\\,{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\).\nProof of Gaussian Tail Inequality\rConsider a univariate \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)\\), then the probability density function is given as \\(\\phi \\left( x \\right)=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}\\).\rLet’s take the derivative w.r.t \\(x\\) we get:\r\\[\\frac{d\\phi \\left( x \\right)}{dx}={\\phi }\u0026#39;\\left( x \\right)=\\frac{d\\left( \\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}\\frac{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}\\left( -x \\right)=-x\\phi \\left( x \\right)\\]\rLet’s define the gaussian tail inequality.\r\\[P\\left( X\u0026gt;\\varepsilon \\right)\\le {{\\varepsilon }^{-1}}\\int_{\\varepsilon }^{\\infty }{s\\phi \\left( s \\right)ds}\\]\r\\[P\\left( X\u0026gt;\\varepsilon \\right)\\le {{\\varepsilon }^{-1}}\\int_{\\varepsilon }^{\\infty }{s\\phi \\left( s \\right)ds}=-{{\\varepsilon }^{-1}}\\int_{\\varepsilon }^{\\infty }{{\\phi }\u0026#39;\\left( s \\right)ds}=-{{\\varepsilon }^{-1}}\\left. {\\phi }\u0026#39;\\left( s \\right) \\right|_{\\varepsilon }^{\\infty }=-{{\\varepsilon }^{-1}}\\left[ {\\phi }\u0026#39;\\left( \\infty \\right)-{\\phi }\u0026#39;\\left( \\varepsilon \\right) \\right]\\]\rWe know that \\(x\\phi \\left( x \\right)=-{\\phi }\u0026#39;\\left( x \\right)\\)\r\\[P\\left( X\u0026gt;\\varepsilon \\right)\\le -{{\\varepsilon }^{-1}}\\left[ 0-{\\phi }\u0026#39;\\left( \\varepsilon \\right) \\right]=\\frac{{\\phi }\u0026#39;\\left( \\varepsilon \\right)}{\\varepsilon }=\\frac{1}{\\varepsilon \\sqrt{2\\pi }}{{e}^{-\\frac{{{\\varepsilon }^{2}}}{2}}}\\le \\frac{{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }\\]\rNow, by the symmetry of distribution,\r\\[P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }\\]\nProof for Gaussian Tail Inequality for distribution of mean\rNow, let’s consider \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)\\) and \\({{\\bar{X}}_{n}}={{n}^{-1}}\\sum\\limits_{i=1}^{n}{{{x}_{i}}}\\sim N\\left( 0,{{n}^{-1}} \\right)\\) therefore, \\({{\\bar{X}}_{n}}\\overset{d}{\\mathop{=}}\\,{{n}^{-{1}/{2}\\;}}Z\\) where \\(Z\\sim N\\left( 0,1 \\right)\\) and by Gaussian Tail Inequalities\r\\[P\\left( \\left| {{{\\bar{X}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)=P\\left( {{n}^{-{1}/{2}\\;}}\\left| Z \\right|\u0026gt;\\varepsilon \\right)=P\\left( \\left| Z \\right|\u0026gt;\\sqrt{n}\\varepsilon \\right)\\le \\frac{2}{\\sqrt{n}\\varepsilon }{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\]\nExercise:\rImagine \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,{{\\sigma }^{2}} \\right)\\)and prove the gaussian tail inequality that \\[P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{{{\\sigma }^{2}}}{\\varepsilon }\\frac{1}{\\sqrt{2\\pi {{\\sigma }^{2}}}}2{{e}^{-{{{\\varepsilon }^{2}}}/{\\left( 2{{\\sigma }^{2}} \\right)}\\;}}\\]\nTheorem 2: Markov’s Inequality\rLet \\(X\\) be a non-negative random variable and \\(E\\left( X \\right)\\) exists, For any \\(t\u0026gt;0\\); \\(P\\left( X\u0026gt;t \\right)\\le \\frac{E\\left( X \\right)}{t}\\)\nProof of Markov’s Inequality\rFor \\(X\u0026gt;0\\) we can write expectation of \\(X\\) as:\r\\[E\\left( X \\right)=\\int\\limits_{0}^{\\infty }{xp\\left( x \\right)dx}=\\int\\limits_{0}^{t}{xp\\left( x \\right)dx}+\\int\\limits_{t}^{\\infty }{xp\\left( x \\right)dx}\\ge \\int\\limits_{t}^{\\infty }{xp\\left( x \\right)dx}\\]\r\\[E\\left( X \\right)\\ge \\int\\limits_{t}^{\\infty }{xp\\left( x \\right)dx}\\ge t\\int\\limits_{t}^{\\infty }{p\\left( x \\right)dx}=tP\\left( X\u0026gt;t \\right)\\]\r\\[\\frac{E\\left( X \\right)}{t}\\ge P\\left( X\u0026gt;t \\right)\\]\r\\[P\\left( X\u0026gt;t \\right)\\le \\frac{E\\left( X \\right)}{t}\\]\nTheorem 3: Chebyshev’s Inequality\rLet \\(\\mu =E\\left( X \\right)\\) and \\(Var\\left( X \\right)={{\\sigma }^{2}}\\), then \\(P\\left( \\left| X-\\mu \\right|\\ge t \\right)\\le \\frac{{{\\sigma }^{2}}}{{{t}^{2}}}\\) and \\(P\\left( \\left| Z \\right|\\ge k \\right)\\le \\frac{1}{{{k}^{2}}}\\)where \\(Z=\\frac{X-\\mu }{{{\\sigma }^{2}}}\\) and in particular \\(P\\left( \\left| Z \\right|\u0026gt;2 \\right)\\le \\frac{1}{4}\\) and \\(P\\left( \\left| Z \\right|\u0026gt;3 \\right)\\le \\frac{1}{9}\\).\nProof of Chebyshev’s Inequality\rLet’s take \\[P\\left( \\left| X-\\mu \\right|\u0026gt;t \\right)=P\\left( {{\\left| X-\\mu \\right|}^{2}}\u0026gt;{{t}^{2}} \\right)\\le \\frac{E{{\\left( X-\\mu \\right)}^{2}}}{{{t}^{2}}}=\\frac{{{\\sigma }^{2}}}{{{t}^{2}}}\\]\rLet’s take \\[P\\left( \\left| \\frac{X-\\mu }{\\sigma } \\right|\u0026gt;\\sigma k \\right)=P\\left( {{\\left| \\frac{X-\\mu }{\\sigma } \\right|}^{2}}\u0026gt;{{\\sigma }^{2}}{{k}^{2}} \\right)\\le \\frac{E{{\\left( X-\\mu \\right)}^{2}}}{{{\\sigma }^{2}}{{k}^{2}}}=\\frac{{{\\sigma }^{2}}}{{{\\sigma }^{2}}{{k}^{2}}}=\\frac{1}{{{k}^{2}}}\\]\nTheorem 4: Hoeffding Inequality\rIn probability theory, Hoeffding’s inequality provides an upper bound on the probability that the sum of bounded independent random variables deviates from its expected value by more than a certain amount. Hoeffding’s inequality was proven by Wassily Hoeffding in 1963. This inequality is sharper than Markov inequality and we can create upper bound without knowing the variance.\nIf \\(a\u0026lt;X\u0026lt;b\\) and \\(\\mu =E\\left( X \\right)\\) then \\(P\\left( \\left| X-\\mu \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\).\nProof (part-A)\rLet’s assume \\(\\mu =0\\). If data is don’t have \\(\\mu =0\\), we can always center the data and \\(a\u0026lt;X\u0026lt;b\\). Now\n\\[X=\\gamma a+\\left( 1-\\gamma \\right)b\\] where \\(0\u0026lt;\\gamma \u0026lt;1\\) and \\(\\gamma =\\frac{X-a}{b-a}\\). With convexity we can write:\r\\[{{e}^{tX}}\\le \\gamma {{e}^{tb}}+\\left( 1-\\gamma \\right){{e}^{ta}}=\\frac{X-a}{b-a}{{e}^{tb}}+\\left( 1-\\frac{X-a}{b-a} \\right){{e}^{ta}}=\\frac{X-a}{b-a}{{e}^{tb}}+\\frac{b-X}{b-a}{{e}^{ta}}\\]\r\\[{{e}^{tX}}\\le \\frac{X{{e}^{tb}}-a{{e}^{tb}}+b{{e}^{ta}}-X{{e}^{ta}}}{b-a}=\\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+\\frac{X\\left( {{e}^{tb}}-{{e}^{ta}} \\right)}{b-a}\\]\rLet’s take the expectation on the both sides:\r\\[E\\left( {{e}^{tX}} \\right)\\le E\\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+E\\frac{X\\left( {{e}^{tb}}-{{e}^{ta}} \\right)}{b-a}=\\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+\\frac{\\left( {{e}^{tb}}-{{e}^{ta}} \\right)}{b-a}E\\left( X \\right)\\]\rSince \\(\\mu =E\\left( X \\right)=0\\) therefore,\r\\[E\\left( {{e}^{tX}} \\right)\\le \\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)+0\\]\r\\[E\\left( {{e}^{tX}} \\right)\\le \\left( \\frac{-a{{e}^{tb}}+b{{e}^{ta}}}{b-a} \\right)=\\frac{{{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{b-a}\\]\nLet’s define\r\\[{{e}^{g\\left( t \\right)}}=\\frac{{{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{b-a}\\]\nTaking \\(log\\) on the both sides:\n\\[\\log \\left( {{e}^{g\\left( t \\right)}} \\right)=\\log \\left( \\frac{{{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{b-a} \\right)\\]\n\\[g\\left( t \\right)=\\log \\left( {{e}^{ta}}\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right) \\right)-\\log \\left( b-a \\right)\\]\n\\[g\\left( t \\right)=\\log \\left( {{e}^{ta}} \\right)+\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)-\\log \\left( b-a \\right)\\]\n\\[\\begin{equation}\rg\\left( t \\right)=ta+\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)-\\log \\left( b-a \\right)\r\\end{equation}\\]\nTaylor series expansion\rFor a univariate function \\(g(x)\\)evaluated at \\({{x}_{0}}\\) , we can express with Taylor series expansion as:\r\\[g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\\cdots +\\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\\frac{1}{(m)!}{{g}^{(m)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m}}+\\cdots \\]\rFor a univariate function \\(g(x)\\)evaluated at \\({{x}_{0}}\\) that is \\(m\\) times differentiable, we can express with Taylor series expansion as:\r\\[g(x)=g({{x}_{0}})+{{g}^{(1)}}({{x}_{0}})(x-{{x}_{0}})+\\frac{1}{2!}{{g}^{(2)}}({{x}_{0}}){{(x-{{x}_{0}})}^{2}}+\\cdots +\\frac{1}{(m-1)!}{{g}^{(m-1)}}({{x}_{0}}){{(x-{{x}_{0}})}^{m-1}}+\\frac{1}{(m)!}{{g}^{(m)}}(\\xi ){{(x-{{x}_{0}})}^{m}}\\]\rwhere \\({{g}^{(s)}}={{\\left. \\frac{{{\\partial }^{s}}g(x)}{\\partial {{x}^{2}}} \\right|}_{x={{x}_{0}}}}\\) and and \\(\\xi\\) lies between \\(x\\) and \\({{x}_{0}}\\).\nProof (part-B)\rNow, let’s evaluate \\(g\\left( t=0 \\right)\\) we get:\r\\[\\begin{equation}\rg\\left( t=0 \\right)=g\\left( 0 \\right)=0+\\log \\left( b-a \\right)-\\log \\left( b-a \\right)=0\r\\end{equation}\\]\nLet’s evaluate \\({g}\u0026#39;\\left( t=0 \\right)\\) but before that lets find \\({g}\u0026#39;\\left( t \\right)\\).\r\\[\\frac{dg\\left( t \\right)}{dt}={g}\u0026#39;\\left( t \\right)=\\frac{d\\left( ta+\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)-\\log \\left( b-a \\right) \\right)}{dt}\\]\r\\[{g}\u0026#39;\\left( t \\right)=\\frac{d\\left( ta \\right)}{dt}+\\frac{d\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{dt}+\\frac{d\\left( -\\log \\left( b-a \\right) \\right)}{dt}\\]\r\\[{g}\u0026#39;\\left( t \\right)=a+\\frac{d\\log \\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{d\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}\\frac{d\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right)}{dt}+\\underbrace{\\frac{d\\left( -\\log \\left( b-a \\right) \\right)}{dt}}_{0}\\]\r\\[{g}\u0026#39;\\left( t \\right)=a+\\frac{-a\\left( b-a \\right){{e}^{t\\left( b-a \\right)}}}{b-a{{e}^{t\\left( b-a \\right)}}}\\]\rConsider the second term:\r\\[\\frac{-a\\left( b-a \\right){{e}^{t\\left( b-a \\right)}}}{b-a{{e}^{t\\left( b-a \\right)}}}=\\frac{-a\\left( b-a \\right)}{\\left( b-a{{e}^{t\\left( b-a \\right)}} \\right){{e}^{-t\\left( b-a \\right)}}}=\\frac{-a\\left( b-a \\right)}{b{{e}^{-t\\left( b-a \\right)}}-a{{e}^{t\\left( b-a \\right)}}{{e}^{-t\\left( b-a \\right)}}}=\\frac{-a\\left( b-a \\right)}{b{{e}^{-t\\left( b-a \\right)}}-a}=\\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}}\\]\r\\[{g}\u0026#39;\\left( t \\right)=a+\\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}}\\]\rNow Let’s evaluate \\({g}\u0026#39;\\left( t=0 \\right)\\), we get\r\\[\\begin{equation}\r{g}\u0026#39;\\left( t=0 \\right)=a+\\frac{-a\\left( b-a \\right){{e}^{0\\left( b-a \\right)}}}{b-a{{e}^{0\\left( b-a \\right)}}}=a+\\frac{-a\\left( b-a \\right)}{b-a}=0\r\\end{equation}\\]\nNow let’s take\\({{g}\u0026#39;}\u0026#39;\\left( t \\right)\\).\n\\[\\frac{d{g}\u0026#39;\\left( t \\right)}{dt}={{g}\u0026#39;}\u0026#39;\\left( t \\right)=\\frac{d\\left( a+\\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}} \\right)}{dt}=\\frac{d\\left( \\frac{a\\left( b-a \\right)}{a+b{{e}^{-t\\left( b-a \\right)}}} \\right)}{dt}\\]\n\\[{{g}\u0026#39;}\u0026#39;\\left( t \\right)=\\frac{-a\\left( b-a \\right)\\left( -b \\right)\\left( -\\left( b-a \\right){{e}^{-t\\left( b-a \\right)}} \\right)}{{{\\left( a+b{{e}^{-t\\left( b-a \\right)}} \\right)}^{2}}}=\\frac{ab{{\\left( b-a \\right)}^{2}}\\left[ -{{e}^{t\\left( b-a \\right)}} \\right]}{{{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}}\\]\nNow we can compare following two terms.\r\\[a{{e}^{t\\left( b-a \\right)}}\\ge a\\]\nNegate \\(b\\) and square on the both sides:\n\\[{{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}\\ge {{\\left( a-b \\right)}^{2}}={{\\left( b-a \\right)}^{2}}\\]\n\\[\\frac{1}{{{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}}\\le \\frac{1}{{{\\left( b-a \\right)}^{2}}}\\]\nFrom above inequality, we can write:\n\\[{{g}\u0026#39;}\u0026#39;\\left( t \\right)=\\frac{-ab{{\\left( b-a \\right)}^{2}}\\left[ {{e}^{t\\left( b-a \\right)}} \\right]}{{{\\left( a{{e}^{t\\left( b-a \\right)}}-b \\right)}^{2}}}\\le \\frac{-ab{{\\left( b-a \\right)}^{2}}}{{{\\left( b-a \\right)}^{2}}}\\]\n\\[\\begin{equation}\r{g}\u0026#39;\u0026#39;\\left( t \\right)\\le -ab=\\frac{{{\\left( a-b \\right)}^{2}}-{{\\left( b-a \\right)}^{2}}}{4}\\le \\frac{{{\\left( b-a \\right)}^{2}}}{4}\r\\end{equation}\\]\nNow, with Taylor series expansion we have:\n\\[g\\left( t \\right)=g\\left( 0 \\right)+t{g}\u0026#39;\\left( 0 \\right)+\\frac{1}{2!}{{t}^{2}}{{g}\u0026#39;}\u0026#39;\\left( 0 \\right)+\\cdots\\]\nAnd with truncating Taylor series expansion, we can write. (Note this is not approximation, its exact)\n\\[g\\left( t \\right)=g\\left( 0 \\right)+t{g}\u0026#39;\\left( 0 \\right)+\\frac{1}{2!}{{t}^{2}}{{g}\u0026#39;}\u0026#39;\\left( \\xi \\right)=\\frac{1}{2!}{{t}^{2}}{{g}\u0026#39;}\u0026#39;\\left( \\xi \\right)\\le \\frac{1}{2!}{{t}^{2}}\\frac{{{\\left( b-a \\right)}^{2}}}{4}\\]\n\\[\\begin{equation}\rg\\left( t \\right)\\le \\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}\r\\end{equation}\\]\nProof (part-C)\rWe have bound \\(E\\left[ {{e}^{tX}} \\right]\\le {{e}^{g\\left( t \\right)}}\\) and \\({{e}^{g\\left( t \\right)}}\\le {{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}\\) .\nConsider\r\\[P\\left( X\u0026gt;\\varepsilon \\right)=P\\left( {{e}^{X}}\u0026gt;{{e}^{\\varepsilon }} \\right)=P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)\\]\nAnd Now with Markov inequality:\n\\[P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)\\le \\frac{E\\left( {{e}^{tX}} \\right)}{{{e}^{t\\varepsilon }}}={{e}^{-t\\varepsilon }}E\\left( {{e}^{tX}} \\right)\\le {{e}^{-t\\varepsilon }}{{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}={{e}^{^{-t\\varepsilon +\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}}\\]\nNow we can make it sharper by following argument:\n\\[P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)\\le \\underset{t\\ge 0}{\\mathop{\\inf }}\\,\\frac{E\\left( {{e}^{tX}} \\right)}{{{e}^{t\\varepsilon }}}={{e}^{-t\\varepsilon }}E\\left( {{e}^{tX}} \\right)\\le {{e}^{-t\\varepsilon }}{{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}={{e}^{^{-t\\varepsilon +\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}}\\]\nProof for sharper version with Chernoff’s method\rlet define: \\(u=t\\varepsilon -\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}\\) and find the minima as setting FOC as \\({u}\u0026#39;\\left( t \\right)=\\varepsilon -\\frac{2t{{\\left( b-a \\right)}^{2}}}{8}\\overset{set}{\\mathop{=}}\\,0\\) and \\({{t}^{*}}=\\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}\\) then substituting to get:\n\\[{{u}_{\\min }}=t\\varepsilon -\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}=\\varepsilon \\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}-{{\\left( \\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}} \\right)}^{2}}\\frac{{{\\left( b-a \\right)}^{2}}}{8}\\]\n\\[{{u}_{\\min }}=\\varepsilon \\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}=\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}\\]\nThe reason we want to get \\({{u}_{\\min }}\\) is to make sharper argument for the inequality or alternatively we would like to bound for the minima. Now substituting:\n\\[P\\left( X\u0026gt;\\varepsilon \\right)=P\\left( {{e}^{X}}\u0026gt;{{e}^{\\varepsilon }} \\right)=P\\left( {{e}^{tX}}\u0026gt;{{e}^{t\\varepsilon }} \\right)\\le {{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\]\n\\[P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\]\nThis is very important results, because there is no mean or variance, so this result is very cogitative. If we observe any type of random variable whose functional form is unknown, the above statement is true.\nProof for random variable with non zero mean.\rNow we can apply with the mean ie. \\(\\mu =E\\left( X \\right)\\) and \\(Y=x-\\mu\\) i.e. \\(a-\\mu \u0026lt;Y\u0026lt;b-\\mu\\). And:\n\\[P\\left( \\left| Y \\right|\u0026gt;\\varepsilon \\right)=P\\left( \\left| X-\\mu \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-\\mu -a+\\mu \\right)}^{2}}}}}=2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\]\nSo, \\(P\\left( \\left| X-\\mu \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\) is known as Hoeffding’s Inequality. This shows that the variation of the random variable beyond its mean by certain amount \\(\\varepsilon\\) is upper bounded by \\(2{{e}^{-\\frac{2{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\). This is true for any random variable so it’s very powerful generalization.\nProof for bound of mean\rLet’s define \\({{\\bar{Y}}_{n}}=\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}\\) and \\({{Y}_{i}}\\) are i.id then let’s bound it as:\n\\[P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)=P\\left( {{n}^{-1}}\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}\u0026gt;\\varepsilon \\right)=P\\left( \\sum\\limits_{i=1}^{n}{{{Y}_{i}}}\u0026gt;n\\varepsilon \\right)=P\\left( {{e}^{\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}}\u0026gt;{{e}^{n\\varepsilon }} \\right)=P\\left( {{e}^{t\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}}\u0026gt;{{e}^{tn\\varepsilon }} \\right)\\]\nNote, we introduce \\(t\\) there that’s for the flexibility that later, I can choose \\(t\\). Now with Markov inequality we can write under the assumption of i.i.d of \\({{Y}_{i}}\\)\n\\[P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)=P\\left( {{e}^{t\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}}\u0026gt;{{e}^{tn\\varepsilon }} \\right)\\le {{e}^{-tn\\varepsilon }}E\\left[ {{e}^{t\\sum\\limits_{i=1}^{n}{{{Y}_{i}}}}} \\right]={{e}^{-tn\\varepsilon }}{{\\left( E{{e}^{t{{Y}_{i}}}} \\right)}^{n}}\\]\nSince, we have bound \\(E\\left[ {{e}^{tX}} \\right]\\le {{e}^{g\\left( t \\right)}}\\) as \\({{e}^{g\\left( t \\right)}}\\le {{e}^{\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}\\) , therefore,\n\\[P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)\\le {{e}^{-tn\\varepsilon }}{{\\left( E{{e}^{t{{Y}_{i}}}} \\right)}^{n}}\\le {{e}^{-tn\\varepsilon }}{{e}^{n\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}\\]\nLet’s try to put sharper bound and try and solve for \\(u\\left( t \\right)=tn\\varepsilon -n\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}\\) and the FOC is\r\\({u}\u0026#39;\\left( t \\right)=n\\varepsilon -n\\frac{2t{{\\left( b-a \\right)}^{2}}}{8}\\overset{set}{\\mathop{=}}\\,0\\) and solving we get \\[{{t}^{*}}=\\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}\\] and the plugging the value of \\({{t}^{*}}\\) on \\(u\\left( t \\right)\\) gives:\n\\[{{u}_{\\min }}={{t}^{*}}n\\varepsilon -n\\frac{{{t}^{*}}^{2}{{\\left( b-a \\right)}^{2}}}{8}=\\frac{4\\varepsilon }{{{\\left( b-a \\right)}^{2}}}n\\varepsilon -n\\frac{{{\\left( 4\\varepsilon \\right)}^{2}}}{{{\\left( {{\\left( b-a \\right)}^{2}} \\right)}^{2}}}\\frac{{{\\left( b-a \\right)}^{2}}}{8}=\\frac{4n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}-\\frac{2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}=\\frac{2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}\\]\nThen,\n\\[P\\left( {{{\\bar{Y}}}_{n}}\u0026gt;\\varepsilon \\right)\\le \\underset{t\\ge 0}{\\mathop{\\inf }}\\,{{e}^{-tn\\varepsilon }}{{e}^{n\\frac{{{t}^{2}}{{\\left( b-a \\right)}^{2}}}{8}}}={{e}^{\\frac{-2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\]\nThen,\n\\[P\\left( \\left| {{{\\bar{Y}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{\\frac{2n{{\\varepsilon }^{2}}}{{{\\left( b-a \\right)}^{2}}}}}\\]\nHence, this gives the bound on the mean.\nProof for Binominal\rHoeffding’s inequality for the \\({{Y}_{1}}\\sim Ber\\left( p \\right)\\) and it’s upper bound is \\(1\\) and lower bound is \\(0\\) so \\({{\\left( b-a \\right)}^{2}}=1\\) and with Hoeffding inequality \\[P\\left( \\left| {{{\\bar{X}}}_{n}}-p \\right|\u0026gt;\\varepsilon \\right)\\le 2{{e}^{-2n{{\\varepsilon }^{2}}}}\\]\nTheorem 5: Kullback Leibler Distance\rProof for distance between density is greater than zero.\rProof that the distance between any two density \\(p\\) and \\(q\\) whose random variable is \\(X\\tilde{\\ }p\\) (\\(p\\) is some distribution) is always greater than or equal to zero.\nPrior answering this, let’s quickly note two inequality, namely Cauchy-Swartz Inequality and Jensen’s inequality.\nCauchy-Swartz Inequality\r\\(\\left| EXY \\right|\\le E\\left| XY \\right|\\le \\sqrt{E\\left( {{X}^{2}} \\right)}\\sqrt{E\\left( {{Y}^{2}} \\right)}\\).\nJensen’s inequality\rIf \\(g\\) is convex then\\(Eg\\left( X \\right)\\ge g\\left( EX \\right)\\). If \\(g\\) is concave, then\\(Eg\\left( X \\right)\\le g\\left( EX \\right)\\).\nKullback Leibler Distance\rThe distance between two density \\(p\\) and \\(q\\) is defined by the Kullback Leibler Distance, and given as:\n\\[D\\left( p,q \\right)=\\int{p\\left( x \\right)\\log \\left( \\frac{p\\left( x \\right)}{q\\left( x \\right)} \\right)}dx\\]\nBefore, I move ahead, note that the self-distance between density \\(p\\) to \\(p\\) is zero and given as: \\(D\\left( p,p \\right)=0\\) and by definition distance is always greater than and equal to zero so, one thing we have to confirm is that distance between two density \\(p\\) and \\(q\\) i.e. \\(D\\left( p,q \\right)\\ge 0\\). But we will use Jensen inequality to proof \\(D\\left( p,q \\right)\\ge 0\\). Since the \\(\\log\\) function is concave in nature, so we can write, Jensen inequality that:\n\\[-D\\left( p,q \\right)=E\\left[ \\log \\left( \\frac{p\\left( x \\right)}{q\\left( x \\right)} \\right) \\right]\\le \\log \\left[ E\\left( \\frac{p\\left( x \\right)}{q\\left( x \\right)} \\right) \\right]=\\log \\int{p\\left( x \\right)\\frac{q\\left( x \\right)}{p\\left( x \\right)}dx}=\\log \\int{q\\left( x \\right)dx}=\\log \\left( 1 \\right)=0\\]\ni.e\n\\[-D\\left( p,q \\right)\\le 0\\] i.e. \\[D\\left( p,q \\right)\\ge 0\\]\nTheorem 6: Maximum of a random variable\rLet \\({{X}_{i}},\\ldots {{X}_{n}}\\) be random variable. Suppose there exist \\(\\sigma \u0026gt;0\\) such that \\(E\\left( {{e}^{t{{X}_{i}}}} \\right)\\le {{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\). Then, \\(E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\sigma \\sqrt{2\\log n}\\).\nMaximum of a random variable represents how to bound maximum value of a random variable? i.e. Say the random variable is \\({{X}_{1}},\\ldots ,{{X}_{n}}\\) and say it is arranged in ascending order such that \\({{X}_{\\left( 1 \\right)}}\\le {{X}_{\\left( 2 \\right)}}\\le \\ldots \\le {{X}_{\\left( n \\right)}}\\) and \\({{X}_{\\left( n \\right)}}={{E}_{\\max }}\\left\\{ {{X}_{1}},\\cdots ,{{X}_{n}} \\right\\}\\)how to compute the distribution of that maximum value? Now the interesting thing is, say, that we don’t know the exact distribution of \\(X\\), so can we say in general without knowing the distribution that what is the maximum of a random variable?\nLet’s start with the expectation of the moment generating function given as: \\(E{{e}^{t{{X}_{i}}}}\\) then, it is bounded by \\(E{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\) i.e. \\(E{{e}^{t{{X}_{i}}}}\\le E{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\). Now, can we bound the maximum of \\(E{{e}^{t{{X}_{i}}}}\\)or alternatively, what is the \\(E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\) ?\nLet’s, start with \\(E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\) and pre-multiply this with \\(t\\) and exponentiate. i.e. \\(\\exp \\left\\{ tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}} \\right\\}\\), bounding this gives also is same as bounding \\(E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\). Now with Jensen’s inequality we can write:\n\\[{{e}^{tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}}}\\le E{{e}^{t\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}}}=E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{e}^{t{{X}_{i}}}}\\le \\sum\\limits_{i=1}^{n}{E{{e}^{t{{X}_{i}}}}}\\le n{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\]\r\\[{{e}^{tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}}}\\le n{{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\]\rTaking \\(\\log\\) on the both sides\r\\[tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\log n+\\log {{e}^{\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}}}\\]\r\\[tE\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\log n+\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}\\]\rDividing both sides by \\(t\\), we get\r\\[E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\frac{\\log n}{t}+\\frac{t{{\\sigma }^{2}}}{2}\\]\rNow, let’s take this \\(\\frac{\\log n}{t}+\\frac{t{{\\sigma }^{2}}}{2}\\) and optimize w.r.t \\(t\\) we get: \\(\\log n=\\frac{{{t}^{2}}{{\\sigma }^{2}}}{2}\\) and \\(t={{\\sigma }^{-1}}\\sqrt{2\\log n}\\). Now plugging this value, we get:\n\\[E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\frac{\\log n}{t}+\\frac{t{{\\sigma }^{2}}}{2}=\\frac{2\\log n+{{t}^{2}}{{\\sigma }^{2}}}{2t}=\\frac{2\\log n+{{\\left( {{\\sigma }^{-1}}\\sqrt{2\\log n} \\right)}^{2}}{{\\sigma }^{2}}}{2{{\\sigma }^{-1}}\\sqrt{2\\log n}}\\]\r\\[E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\frac{2\\log n+{{\\sigma }^{-2}}2\\log n{{\\sigma }^{2}}}{2{{\\sigma }^{-1}}\\sqrt{2\\log n}}=\\frac{2\\sqrt{2}\\sqrt{2}\\log n}{2{{\\sigma }^{-1}}\\sqrt{2}\\sqrt{\\log n}}=\\sigma \\sqrt{2\\log n}\\]\rHence\r\\[E\\underset{1\\le i\\le n}{\\mathop{\\max }}\\,{{X}_{i}}\\le \\sigma \\sqrt{2\\log n}\\]\n","date":1545782400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545782400,"objectID":"91e2a476693769ccbc5adc50e9309aae","permalink":"https://ShishirShakya.github.io/post/2018-12-29-proof-of-hoeffding-inequality/","publishdate":"2018-12-26T00:00:00Z","relpermalink":"/post/2018-12-29-proof-of-hoeffding-inequality/","section":"post","summary":"Theorem 1: Gaussian Tail Inequality\rGiven \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)\\) then, \\(P\\left( \\left| X \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2{{e}^{-{{{\\varepsilon }^{2}}}/{2}\\;}}}{\\varepsilon }\\) and \\(P\\left( \\left| {{{\\bar{X}}}_{n}} \\right|\u0026gt;\\varepsilon \\right)\\le \\frac{2}{\\sqrt{n}\\varepsilon }{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\overset{l\\arg e\\ n}{\\mathop{\\le }}\\,{{e}^{-{n{{\\varepsilon }^{2}}}/{2}\\;}}\\).\nProof of Gaussian Tail Inequality\rConsider a univariate \\({{x}_{1}},\\cdots ,{{x}_{n}}\\sim N\\left( 0,1 \\right)\\), then the probability density function is given as \\(\\phi \\left( x \\right)=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}\\).\rLet’s take the derivative w.r.t \\(x\\) we get:\r\\[\\frac{d\\phi \\left( x \\right)}{dx}={\\phi }\u0026#39;\\left( x \\right)=\\frac{d\\left( \\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}\\frac{d\\left( \\,{{e}^{-\\frac{{{x}^{2}}}{2}}} \\right)}{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}\\frac{d\\left( -\\frac{{{x}^{2}}}{2} \\right)}{dx}=\\frac{1}{\\sqrt{2\\pi }}{{e}^{-\\frac{{{x}^{2}}}{2}}}\\left( -x \\right)=-x\\phi \\left( x \\right)\\]\rLet’s define the gaussian tail inequality.","tags":["Probability,","R,","Statistics,","Theory,","Proof,"],"title":"Probability Inequality","type":"post"}]